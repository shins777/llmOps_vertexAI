{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc17177-c22f-4e21-a383-6bfbab7803f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright  2024 Forusone\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb25396-c162-47f0-adf5-10a155411d59",
   "metadata": {},
   "source": [
    "## deepspeed on Local Ray\n",
    "\n",
    "* https://docs.ray.io/en/latest/train/examples/deepspeed/gptj_deepspeed_fine_tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae11b8-9141-41ce-9991-359aa2f6cc59",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcda7da-c4d9-4e29-b265-ff01effd405f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --user -q \"google-cloud-aiplatform[ray]>=1.56.0\" \\\n",
    "                        \"ray[data,train,tune,serve]>=2.9.3\" \\\n",
    "                          \"transformers==4.27.4\" \\\n",
    "                          \"deepspeed>=0.14.4\" \\\n",
    "                          \"torch==2.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80c6b95-e6eb-4fb5-979b-847b94d4f467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import ray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "567a97a5-5051-4be5-8131-52564a39a13f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "use_gpu = True\n",
    "num_workers = 2\n",
    "cpus_per_worker = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812559c3-0ce7-4d72-b3b3-11ccc3fa9483",
   "metadata": {},
   "source": [
    "### Connnect to Local Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5abd898f-5df7-4c8d-843e-38ce1167b541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ab8495e-596f-4152-9bd1-b162ea676ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUNTIME_ENV = {\n",
    "  \"pip\": [\n",
    "      \"google-cloud-aiplatform[ray]>=1.56.0\",\n",
    "      \"ray[data,train,tune,serve]>=2.9.3\",\n",
    "      \"datasets\",\n",
    "      \"evaluate\",\n",
    "      \"accelerate==0.18.0\",\n",
    "      \"transformers==4.26.0\",\n",
    "      \"torch>=1.12.0\",\n",
    "      \"deepspeed==0.12.3\",\n",
    "      \"setuptools\"\n",
    "  ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09947d-9554-40aa-9327-5624777efb9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ray.init(\n",
    "    runtime_env=RUNTIME_ENV,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27be9936-2229-458e-b0d1-1598f4ad4869",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.3'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80db6e-2f44-422d-9ec2-25d33f502aa2",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9414b6da-dd32-4c47-8bee-1a41b84965bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tiny_shakespeare dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading tiny_shakespeare dataset\")\n",
    "current_dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "current_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4abaadf-651e-424f-9bb4-106c0ad011a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "slice_dataset = DatasetDict({'train': current_dataset['train'].select(range(2000))})\n",
    "slice_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b154b8dd-778c-4b3d-abd7-58bdf5d76fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, validation_dataset= slice_dataset['train'].train_test_split(test_size=0.1).values()\n",
    "dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bf83a38c-6d66-4cad-9468-2f32f3c5afa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets.dataset_dict import DatasetDict\n",
    "# from datasets import Dataset\n",
    "\n",
    "# train_dataset, validation_dataset= current_dataset['train'].train_test_split(test_size=0.1).values()\n",
    "\n",
    "# dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset})\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b69f5c20-1acd-41e8-bd7f-7565ef8c9be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': MaterializedDataset(\n",
       "    num_blocks=1,\n",
       "    num_rows=1800,\n",
       "    schema={instruction: string, input: string, output: string, text: string}\n",
       " ),\n",
       " 'validation': MaterializedDataset(\n",
       "    num_blocks=1,\n",
       "    num_rows=200,\n",
       "    schema={instruction: string, input: string, output: string, text: string}\n",
       " )}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray.data\n",
    "\n",
    "ray_datasets = {\n",
    "    \"train\": ray.data.from_huggingface(dataset[\"train\"]),\n",
    "    \"validation\": ray.data.from_huggingface(dataset[\"validation\"]),\n",
    "}\n",
    "\n",
    "ray_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b95cdd7e-798b-4f10-befe-6ffc279ddfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1455f72c-fd2a-4efb-ad4b-152d00ed22b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': MapBatches(tokenize)\n",
       " +- MapBatches(split_text)\n",
       "    +- Dataset(\n",
       "          num_blocks=1,\n",
       "          num_rows=1800,\n",
       "          schema={\n",
       "             instruction: string,\n",
       "             input: string,\n",
       "             output: string,\n",
       "             text: string\n",
       "          }\n",
       "       ),\n",
       " 'validation': MapBatches(tokenize)\n",
       " +- MapBatches(split_text)\n",
       "    +- Dataset(\n",
       "          num_blocks=1,\n",
       "          num_rows=200,\n",
       "          schema={\n",
       "             instruction: string,\n",
       "             input: string,\n",
       "             output: string,\n",
       "             text: string\n",
       "          }\n",
       "       )}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_text(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    text = list(batch[\"text\"])\n",
    "    flat_text = \"\".join(text)\n",
    "    split_text = [\n",
    "        x.strip()\n",
    "        for x in flat_text.split(\"\\n\")\n",
    "        if x.strip() and not x.strip()[-1] == \":\"\n",
    "    ]\n",
    "    return pd.DataFrame(split_text, columns=[\"text\"])\n",
    "\n",
    "\n",
    "def tokenize(batch: pd.DataFrame) -> dict:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"text\"]),\n",
    "        truncation=True,\n",
    "        max_length=block_size,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    return dict(ret)\n",
    "\n",
    "\n",
    "processed_datasets = {\n",
    "    key: (\n",
    "        ds.map_batches(split_text, batch_format=\"pandas\")\n",
    "        .map_batches(tokenize, batch_format=\"pandas\")\n",
    "    )\n",
    "    for key, ds in ray_datasets.items()\n",
    "}\n",
    "processed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b0f6c6-9940-4bd6-b0f7-af38e2e95bd0",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f064f44-0998-4579-90f9-c9a8ef28dba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GPTJForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from transformers.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "\n",
    "from ray import train\n",
    "from ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n",
    "\n",
    "\n",
    "def train_func(config):\n",
    "    # Use the actual number of CPUs assigned by Ray\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(\n",
    "        train.get_context().get_trial_resources().bundles[-1].get(\"CPU\", 1)\n",
    "    )\n",
    "    # Enable tf32 for better performance\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    batch_size = config.get(\"batch_size\", 4)\n",
    "    epochs = config.get(\"epochs\", 2)\n",
    "    warmup_steps = config.get(\"warmup_steps\", 0)\n",
    "    learning_rate = config.get(\"learning_rate\", 0.00002)\n",
    "    weight_decay = config.get(\"weight_decay\", 0.01)\n",
    "    steps_per_epoch = config.get(\"steps_per_epoch\")\n",
    "\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 8,\n",
    "            \"hysteresis\": 4,\n",
    "            \"consecutive_hysteresis\": True,\n",
    "        },\n",
    "        \"bf16\": {\"enabled\": \"auto\"},\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": \"auto\",\n",
    "                \"betas\": \"auto\",\n",
    "                \"eps\": \"auto\",\n",
    "            },\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 10,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False,\n",
    "    }\n",
    "\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        \"output\",\n",
    "        logging_steps=1,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=steps_per_epoch,\n",
    "        max_steps=steps_per_epoch * epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        label_names=[\"input_ids\", \"attention_mask\"],\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        deepspeed=deepspeed,\n",
    "    )\n",
    "    disable_progress_bar()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Loading model\")\n",
    "\n",
    "    model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "    enable_progress_bar()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    train_ds = train.get_dataset_shard(\"train\")\n",
    "    eval_ds = train.get_dataset_shard(\"validation\")\n",
    "\n",
    "    train_ds_iterable = train_ds.iter_torch_batches(\n",
    "        batch_size=batch_size,\n",
    "        local_shuffle_buffer_size=train.get_context().get_world_size() * batch_size,\n",
    "    )\n",
    "    eval_ds_iterable = eval_ds.iter_torch_batches(batch_size=batch_size)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds_iterable,\n",
    "        eval_dataset=eval_ds_iterable,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    # Add callback to report checkpoints to Ray Train\n",
    "    trainer.add_callback(RayTrainReportCallback())\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "84d8dee6-0659-4f38-ad96-ce6315399a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "storage_path = \"gs://sllm_checkpoints/tmp_store/deepspeed/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e1ddc5c-b467-496a-b3c1-7173e3fdcb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 12:59:51,509\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(split_text)->MapBatches(tokenize)]\n",
      "2025-02-17 12:59:51,509\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2025-02-17 12:59:51,510\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_ds_size = processed_datasets[\"train\"].count()\n",
    "steps_per_epoch = train_ds_size // (batch_size * num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "499c2091-9d7e-4030-9f2d-7978c1ef8503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import RunConfig, ScalingConfig\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config={\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": batch_size,  # per device\n",
    "        \"steps_per_epoch\": steps_per_epoch,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=use_gpu,\n",
    "        resources_per_worker={\"GPU\": 1, \"CPU\": cpus_per_worker},\n",
    "    ),\n",
    "    datasets=processed_datasets,\n",
    "    run_config=RunConfig(storage_path=storage_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2527a93-6d1c-4080-a792-ae1d349429eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "953f4826-8467-413c-804e-f0af59a87e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-17 13:33:54</td></tr>\n",
       "<tr><td>Running for: </td><td>00:33:14.35        </td></tr>\n",
       "<tr><td>Memory:      </td><td>149.1/188.7 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 5.0/48 CPUs, 2.0/4 GPUs (0.0/1.0 accelerator_type:L4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_303ef_00000</td><td>RUNNING </td><td>10.128.0.4:4109099</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=4110157) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 13:33:54,063\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-02-17 13:34:04,410\tINFO tune.py:1042 -- Total run time: 2004.68 seconds (1994.01 seconds for the tuning loop).\n",
      "2025-02-17 13:34:04,411\tWARNING tune.py:1052 -- Training has been interrupted, but the most recent state was saved.\n",
      "Resume training with: Trainer.restore(path=\"sllm_checkpoints/tmp_store/deepspeed/TorchTrainer_2025-02-17_13-00-39\", ...)\n"
     ]
    }
   ],
   "source": [
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab0f77-695f-43e2-aab2-3435392f14bc",
   "metadata": {},
   "source": [
    "### Generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17aee9e1-9e46-4b36-8596-b25ff0bc2197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = results.checkpoint\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf44981-2661-4c1e-beac-8b347b32120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(f\"aws s3 sync s3://{checkpoint.path} /mnt/local_storage/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b470bcb-9725-49d4-810a-a4200e3ad6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, GPTJForCausalLM\n",
    "\n",
    "model = GPTJForCausalLM.from_pretrained(\"/mnt/local_storage/checkpoint\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/local_storage/checkpoint\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebb4de-6e7f-4c41-9f9b-75ca8bb3caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from prompts!\n",
    "for sentence in pipe(\n",
    "    [\"Romeo and Juliet\", \"Romeo\", \"Juliet\"], do_sample=True, min_length=20\n",
    "):\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2a3b5-c838-458a-84c7-6bc2f03aec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright  2024 Forusone\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb25396-c162-47f0-adf5-10a155411d59",
   "metadata": {},
   "source": [
    "## Deepspeed with Ray on Vertex AI\n",
    "\n",
    "* https://docs.ray.io/en/latest/train/examples/deepspeed/gptj_deepspeed_fine_tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eaaa29-3fe1-40b6-9b05-efef65d263b7",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcda7da-c4d9-4e29-b265-ff01effd405f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --user -q \"google-cloud-aiplatform[ray]>=1.56.0\" \\\n",
    "                        \"ray[data,train,tune,serve]==2.33.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c6b95-e6eb-4fb5-979b-847b94d4f467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62361efc-25f7-4e71-bfd8-feda070088f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed41e8-4912-456f-8ba0-b3ac84a2e545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Define constants\n",
    "PROJECT_NBR = \"721521243942\"\n",
    "PROJECT_ID = \"ai-hangsik\"\n",
    "REGION=\"us-central1\"\n",
    "RAY_CLUSTER_NM = \"ray33-cluster-20250218-085159\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7b2e5-f2e4-44c7-9967-9637b9d5f7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13c1ec-2e9d-428c-aff2-09035dfce1cb",
   "metadata": {},
   "source": [
    "### Connect to Ray on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd898f-5df7-4c8d-843e-38ce1167b541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8495e-596f-4152-9bd1-b162ea676ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAY_ADDRESS=f\"vertex_ray://projects/{PROJECT_NBR}/locations/{REGION}/persistentResources/{RAY_CLUSTER_NM}\"\n",
    "print(f\"RAY_ADDRESS:{RAY_ADDRESS}\")\n",
    "\n",
    "RUNTIME_ENV = {\n",
    "  \"pip\": [\n",
    "      \"google-cloud-aiplatform[ray]>=1.56.0\",\n",
    "      \"ray[data,train,tune,serve]==2.33.0\",\n",
    "      \"datasets\",\n",
    "      \"evaluate\",\n",
    "      \"accelerate==0.18.0\",\n",
    "      \"transformers==4.26.0\",\n",
    "      # \"torch==1.12.0\",\n",
    "      \n",
    "      \"torch==2.0.0\",  # for CUDA 11.8 : https://pytorch.org/get-started/previous-versions/\n",
    "      \"deepspeed==0.14.4\", # https://github.com/huggingface/alignment-handbook/issues/180\n",
    "      \"numpy<2\",\n",
    "      \n",
    "      \"setuptools\",\n",
    "      \"ipython\",\n",
    "      \"scikit-learn\",\n",
    "      \"ninja\",\n",
    "      \"triton<=3.1.0\"  # https://github.com/deepspeedai/DeepSpeed/issues/7028\n",
    "      \n",
    "  ],\n",
    "}\n",
    "\n",
    "# pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1  pytorch-cuda=11.8\n",
    "\n",
    "# runtime_env = {\"env_vars\": {\"NCCL_SOCKET_IFNAME\": \"ens5\"}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f8755-9aea-462e-9b97-a70d6c744adc",
   "metadata": {},
   "source": [
    "### Connect to Ray on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09947d-9554-40aa-9327-5624777efb9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.init(address=RAY_ADDRESS,runtime_env=RUNTIME_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a032e-d15e-4204-9c87-a045ddf0e601",
   "metadata": {},
   "source": [
    "### Model and cluster configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b0ef5-6605-4290-acac-a19cd759b50a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "use_gpu = True\n",
    "num_gpus = 2\n",
    "num_workers = 3\n",
    "cpus_per_worker = 6  # g2-standard-24 : https://cloud.google.com/compute/docs/gpus#l4-gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52accab-46e4-4f36-9eee-3234c320c1d8",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f5c20-1acd-41e8-bd7f-7565ef8c9be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray.data\n",
    "\n",
    "@ray.remote\n",
    "def data_manage():\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "    print(\"Loading tiny_shakespeare dataset\")\n",
    "    current_dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "    slice_dataset = DatasetDict({'train': current_dataset['train'].select(range(2000))})\n",
    "\n",
    "    train_dataset, validation_dataset= slice_dataset['train'].train_test_split(test_size=0.1).values()\n",
    "    dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset})\n",
    "\n",
    "    ray_datasets = {\n",
    "        \"train\": ray.data.from_huggingface(dataset[\"train\"]),\n",
    "        \"validation\": ray.data.from_huggingface(dataset[\"validation\"]),\n",
    "    }\n",
    "\n",
    "    return ray_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c462376-41e9-4f9c-b116-292d1d2e2321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray_datasets = ray.get(data_manage.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b0341-e488-4061-a3a4-835bf1524e8c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cdd7e-798b-4f10-befe-6ffc279ddfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455f72c-fd2a-4efb-ad4b-152d00ed22b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def split_text(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    text = list(batch[\"text\"])\n",
    "    flat_text = \"\".join(text)\n",
    "    split_text = [\n",
    "        x.strip()\n",
    "        for x in flat_text.split(\"\\n\")\n",
    "        if x.strip() and not x.strip()[-1] == \":\"\n",
    "    ]\n",
    "    return pd.DataFrame(split_text, columns=[\"text\"])\n",
    "\n",
    "\n",
    "def tokenize(batch: pd.DataFrame) -> dict:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"text\"]),\n",
    "        truncation=True,\n",
    "        max_length=block_size,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    return dict(ret)\n",
    "\n",
    "\n",
    "processed_datasets = {\n",
    "    key: (\n",
    "        ds.map_batches(split_text, batch_format=\"pandas\")\n",
    "        .map_batches(tokenize, batch_format=\"pandas\")\n",
    "    )\n",
    "    for key, ds in ray_datasets.items()\n",
    "}\n",
    "processed_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f064f44-0998-4579-90f9-c9a8ef28dba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GPTJForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from transformers.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "\n",
    "from ray import train\n",
    "from ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n",
    "\n",
    "def train_func(config):\n",
    "    # Use the actual number of CPUs assigned by Ray\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(\n",
    "        train.get_context().get_trial_resources().bundles[-1].get(\"CPU\", 1)\n",
    "    )\n",
    "    # Enable tf32 for better performance\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    batch_size = config.get(\"batch_size\", 4)\n",
    "    epochs = config.get(\"epochs\", 2)\n",
    "    warmup_steps = config.get(\"warmup_steps\", 0)\n",
    "    learning_rate = config.get(\"learning_rate\", 0.00002)\n",
    "    weight_decay = config.get(\"weight_decay\", 0.01)\n",
    "    steps_per_epoch = config.get(\"steps_per_epoch\")\n",
    "\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 8,\n",
    "            \"hysteresis\": 4,\n",
    "            \"consecutive_hysteresis\": True,\n",
    "        },\n",
    "        \"bf16\": {\"enabled\": \"auto\"},\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": \"auto\",\n",
    "                \"betas\": \"auto\",\n",
    "                \"eps\": \"auto\",\n",
    "            },\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 10,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False,\n",
    "    }\n",
    "\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        \"output\",\n",
    "        logging_steps=1,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=steps_per_epoch,\n",
    "        max_steps=steps_per_epoch * epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        label_names=[\"input_ids\", \"attention_mask\"],\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        deepspeed=deepspeed,\n",
    "    )\n",
    "    disable_progress_bar()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Loading model\")\n",
    "\n",
    "    model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "    enable_progress_bar()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    train_ds = train.get_dataset_shard(\"train\")\n",
    "    eval_ds = train.get_dataset_shard(\"validation\")\n",
    "\n",
    "    train_ds_iterable = train_ds.iter_torch_batches(\n",
    "        batch_size=batch_size,\n",
    "        local_shuffle_buffer_size=train.get_context().get_world_size() * batch_size,\n",
    "    )\n",
    "    eval_ds_iterable = eval_ds.iter_torch_batches(batch_size=batch_size)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds_iterable,\n",
    "        eval_dataset=eval_ds_iterable,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    # Add callback to report checkpoints to Ray Train\n",
    "    trainer.add_callback(RayTrainReportCallback())\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8dee6-0659-4f38-ad96-ce6315399a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "storage_path = \"gs://sllm_checkpoints/tmp_store/deepspeed/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ddc5c-b467-496a-b3c1-7173e3fdcb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 16\n",
    "# train_ds_size = processed_datasets[\"train\"].count()\n",
    "# steps_per_epoch = train_ds_size // (batch_size * num_workers)\n",
    "\n",
    "train_ds_size = 1800    \n",
    "steps_per_epoch = round(train_ds_size / (batch_size * num_workers))\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c2091-9d7e-4030-9f2d-7978c1ef8503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import RunConfig, ScalingConfig\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config={\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": batch_size,  # per device\n",
    "        \"steps_per_epoch\": steps_per_epoch,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=use_gpu,\n",
    "        resources_per_worker={\n",
    "            \"GPU\": num_gpus, \n",
    "            \"CPU\": cpus_per_worker\n",
    "        },\n",
    "    ),\n",
    "    datasets=processed_datasets,\n",
    "    run_config=RunConfig(storage_path=storage_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f4826-8467-413c-804e-f0af59a87e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf9ac0-f763-49cd-9eaa-45a91017bfea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646793f-66fc-429c-945b-b641f6bf4a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a1b37-0e9e-417f-820b-6b8962ff1453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aee9e1-9e46-4b36-8596-b25ff0bc2197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoint = results.checkpoint\n",
    "# checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf44981-2661-4c1e-beac-8b347b32120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.system(f\"aws s3 sync s3://{checkpoint.path} /mnt/local_storage/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e599485-d0ae-403c-9b07-cbc4c0527395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline, AutoTokenizer, GPTJForCausalLM\n",
    "\n",
    "# model = GPTJForCausalLM.from_pretrained(\"/mnt/local_storage/checkpoint\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/mnt/local_storage/checkpoint\")\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     task=\"text-generation\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72caea4f-ec55-4f32-9393-4d382fd7ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate from prompts!\n",
    "# for sentence in pipe(\n",
    "#     [\"Romeo and Juliet\", \"Romeo\", \"Juliet\"], do_sample=True, min_length=20\n",
    "# ):\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a81d94-f941-4085-8741-d9382b9f1bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

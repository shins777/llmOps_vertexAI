{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17cecad-9500-42e6-a7d8-dab12f2702cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --user -q \"google-cloud-aiplatform[ray]>=1.56.0\" \\\n",
    "                        \"ray[data,train,tune,serve]>=2.33.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfaf7ff-0a44-4af8-8aaf-962b4bce8163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Define constants\n",
    "PROJECT_NBR = \"721521243942\"\n",
    "PROJECT_ID = \"ai-hangsik\"\n",
    "REGION=\"us-central1\"\n",
    "RAY_CLUSTER_NM = \"ray33-cluster-20250216-192557\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9a7e4f-38e9-44be-a4da-813979e887f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39db2165-ae5d-4e49-9893-9663d0f816d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.33.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray.runtime_env import RuntimeEnv\n",
    "from ray.air.config import RunConfig\n",
    "from ray.air import CheckpointConfig, ScalingConfig\n",
    "from ray.util.joblib import register_ray\n",
    "\n",
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "356aef17-6fb0-4f05-8798-b591d3864b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7a783-08d4-43a8-ad40-460631f9e4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "RAY_ADDRESS=f\"vertex_ray://projects/{PROJECT_NBR}/locations/{REGION}/persistentResources/{RAY_CLUSTER_NM}\"\n",
    "print(f\"RAY_ADDRESS:{RAY_ADDRESS}\")\n",
    "\n",
    "pip_env = {\n",
    "  \"pip\": [\n",
    "        \"datasets==2.12.0\",\n",
    "        \"evaluate==0.4.0\",\n",
    "        \"accelerate==0.16.0\",\n",
    "        \"transformers==4.26.0\",\n",
    "        \"torch==1.13.0\",\n",
    "        \"deepspeed==0.9.2\",\n",
    "        \"ipython==8.14.0\",\n",
    "        \"numpy<2.0.0\",  # https://github.com/deepspeedai/DeepSpeed/issues/5671\n",
    "        \"python-json-logger\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "conda_env = {\n",
    "    \"conda\": {\n",
    "        \"dependencies\": [\"mpi4py\", \"pip\", pip_env]\n",
    "    }  # pip install mpi4py won't work, use conda install instead\n",
    "}\n",
    "\n",
    "ray.init(address=RAY_ADDRESS,runtime_env=conda_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba918260-4fa0-432d-9c7d-5defc14645c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-16 11:22:01,285] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739704921.236411 1118981 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1739704921.261917 1118981 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "import ray.data\n",
    "import ray\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from transformers.utils.logging import enable_progress_bar\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from ray.train.huggingface.transformers import (\n",
    "    prepare_trainer,\n",
    "    RayTrainReportCallback,\n",
    ")\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import RunConfig, ScalingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7031e7-403b-45ed-9f73-e8b8f5ca4def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"databricks/dolly-v2-3b\"\n",
    "# use_gpu = True\n",
    "# num_workers = 2\n",
    "# cpus_per_worker = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdaa86fd-320f-45fc-b0d0-b0ff65664dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 51760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "current_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d953a665-aaa1-4541-b5a7-dbdc11182fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # ref: https://github.com/tloen/alpaca-lora\n",
    "    if data_point[\"instruction\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "                ### Instruction:\n",
    "                {data_point[\"instruction\"]}\n",
    "\n",
    "                ### Input:\n",
    "                {data_point[\"input\"]}\n",
    "\n",
    "                ### Response:\n",
    "                {data_point[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "                ### Instruction:\n",
    "                {data_point[\"instruction\"]}\n",
    "\n",
    "                ### Response:\n",
    "                {data_point[\"output\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0fe75ff-4e2f-4df2-961c-7ec0c5717767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78b8f2098874c899819d5729daae5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = 0\n",
    "CUTOFF_LEN = 128\n",
    "\n",
    "current_dataset = current_dataset.shuffle().map(\n",
    "    lambda data_point: tokenizer(\n",
    "        generate_prompt(data_point),\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7fa8b5-e936-4b7a-8249-8913d499ada4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25653cc1a85a4848b82789ce4fcd582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MaterializedDataset(\n",
       "   num_blocks=1,\n",
       "   num_rows=51760,\n",
       "   schema={\n",
       "      output: string,\n",
       "      input: string,\n",
       "      instruction: string,\n",
       "      input_ids: list<item: int32>,\n",
       "      attention_mask: list<item: int8>\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_datasets = ray.data.from_huggingface(current_dataset[\"train\"])\n",
    "ray_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2232f13-3064-46d0-a27b-4adf99951d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_init_per_worker(config):\n",
    "    batch_size = config.get(\"batch_size\", 1)\n",
    "    epochs = config.get(\"epochs\", 1)\n",
    "    warmup_steps = config.get(\"warmup_steps\", 0)\n",
    "    learning_rate = config.get(\"learning_rate\", 0.00002)\n",
    "    weight_decay = config.get(\"weight_decay\", 0.01)\n",
    "\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 8,\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": \"auto\",\n",
    "                \"betas\": \"auto\",\n",
    "                \"eps\": \"auto\",\n",
    "            },\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 10,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False,\n",
    "    }\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "    \n",
    "    print(\"Loading model\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        # device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    train_ds_iterable = train_ds.iter_torch_batches(\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    print(f\"batch_size: {batch_size}\")\n",
    "    print(\"Preparing training arguments\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"deepspeed-dolly\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        logging_steps=1,\n",
    "        max_steps=steps_per_epoch * epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_train_epochs=epochs,\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=16,\n",
    "        deepspeed=deepspeed,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = 0\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    enable_progress_bar()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds_iterable,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=False\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    #Add callback to report checkpoints to Ray Train\n",
    "    trainer.add_callback(RayTrainReportCallback())\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    \n",
    "    print(\"Start training\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bac9489a-1bf1-4648-9a3f-80d9c532161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "cpus_per_worker = 4\n",
    "batch_size = 3\n",
    "train_ds_size = ray_datasets.count()\n",
    "\n",
    "steps_per_epoch = train_ds_size // (batch_size * num_workers)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=trainer_init_per_worker,\n",
    "    train_loop_config={\n",
    "        \"batch_size\": batch_size,  # batch_size per device\n",
    "        \"epochs\": 2,\n",
    "        \"steps_per_epoch\": steps_per_epoch,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers= num_workers,\n",
    "        use_gpu=True,\n",
    "        resources_per_worker={\n",
    "            \"GPU\": 1,\n",
    "            \"CPU\": 1,\n",
    "        },  # NOTE: huggingface transformers only support 1 GPU per worker.\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        storage_path = \"gs://sllm_checkpoints/databrics-dolly-v2-3b/\"\n",
    "    ),\n",
    "    datasets={\n",
    "        \"train\": ray_datasets,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889abc2-6a48-48c0-8f8d-5dddc02008fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ed881-35e5-4514-97f5-f0fc54357681",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.metrics     # The metrics reported during training.\n",
    "result.checkpoint  # The latest checkpoint reported during training.\n",
    "result.path        # The path where logs are stored.\n",
    "result.error       # The exception that was raised, if training failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e426327-198c-4828-be4a-54f017f9372e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d729046-84d2-46dd-8d6f-2ba2d4b5ebcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e0d20-21a4-49fc-9d3f-47800408ee04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e850cc8-b203-49a0-a3ce-570a28340bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb805b-6620-4024-afa4-0ed42eb8f1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f375b-0f61-46ba-b8c9-40d34aee843a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e27453-69ea-488b-a623-40cff3f3a2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22645e-3d0b-4a61-89c2-7b3dbecf81f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Get started with Gemma on Ray on Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use Ray on Vertex AI for fine-tuning and serving Gemma on Vertex AI.\n",
    "\n",
    "Learn more about [Ray on Vertex AI](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you'll learn how to distribute Gemma Supervised tuning on Ray on Vertex AI. Furthermore, you'll learn how to deploy the trained model seamlessly for offline predictions using Ray Data on Ray on Vertex AI.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- Ray on Vertex AI\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Ray cluster on Vertex AI\n",
    "- Tune Gemma with Ray Train on Ray on Vertex AI\n",
    "- Serving Gemma with Ray Data for offline predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The [Extreme Summarization (XSum) dataset](https://huggingface.co/datasets/EdinburghNLP/xsum) is a dataset about abstractive single-document summarization systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUKGInbrh0Cw"
   },
   "source": [
    "<b>Note</b>: This tutorial uses the Ray Jobs API via public Ray Dashboard. The Ray dashboard address is accessible from outside the VPC, including the public internet. To learn more about  private versus public connectivity, see the [Private and public connectivity](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster#private_and_public_connectivity) section in the [Create a Ray cluster on Vertex AI](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,artifactregistry.googleapis.com,cloudbuild.googleapis.com).\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-25.0.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch==2.2.0 in /opt/conda/lib/python3.10/site-packages (2.2.0+cu121)\n",
      "Requirement already satisfied: torchvision==0.17.0 in /opt/conda/lib/python3.10/site-packages (0.17.0+cu121)\n",
      "Requirement already satisfied: torchaudio==2.2.0 in /opt/conda/lib/python3.10/site-packages (2.2.0+cu121)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.17.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.17.0) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.17.0) (11.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.17.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.17.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.17.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.17.0) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'torch'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=12.0 -c torch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                                    2.2.1\n",
      "torch-model-archiver                     0.12.0\n",
      "torchaudio                               2.2.1\n",
      "torchmetrics                             1.6.1\n",
      "torchvision                              0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 list |grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  550.90.07  Release Build  (dvs-builder@U16-I2-C05-15-3)  Fri May 31 09:44:37 UTC 2024\n",
      "GCC version:  gcc version 10.2.1 20210110 (Debian 10.2.1-6) \n"
     ]
    }
   ],
   "source": [
    "!cat /proc/driver/nvidia/version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/torch/__init__.py\", line 237, in <module>\n",
      "    from torch._C import *  # noqa: F403\n",
      "ImportError: libnvJitLink.so.12: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import torch;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook.\n",
    "\n",
    "* https://pytorch.org/get-started/previous-versions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    USER = \"--user\"\n",
    "else:\n",
    "    USER = \"\"\n",
    "\n",
    "! pip install -q google-cloud-aiplatform[ray]==1.48.0\n",
    "! pip install -q google-cloud-aiplatform[tensorboard]==1.48.0\n",
    "! pip install -q torch==2.2.1 \\\n",
    "                        torchvision==0.17.1 \\\n",
    "                        torchaudio==2.2.1 \\\n",
    "                        datasets==2.17.0 \\\n",
    "                        transformers==4.38.1 \\\n",
    "                        evaluate==0.4.1 \\\n",
    "                        rouge-score==0.1.2 \\\n",
    "                        nltk==3.8.1 \\\n",
    "                        bitsandbytes==0.42.0 \\\n",
    "                        peft==0.8.2 \\\n",
    "                        accelerate==0.27.1 \\\n",
    "\n",
    "! pip install -q tensorflow==2.15.0 \n",
    "! pip install -q etils==1.5.0 \\\n",
    "                        fsspec==2023.10.0 \\\n",
    "                        gcsfs==2023.10.0\n",
    "\n",
    "! pip install -q nvidia-nvjitlink-cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnvJitLink.so.12: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:237\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    236\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: libnvJitLink.so.12: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CylYbUxrx3W-"
   },
   "source": [
    "### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Project ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fPrDj6HE9_EU"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "You create a timestamp to make resources you create unique in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W6Le1schAziq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = f\"20250210_ray_vertexai\"  # @param {type:\"string\"}\n",
    "\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://20250210_ray_vertexai/...\n",
      "ServiceException: 409 A Cloud Storage bucket named '20250210_ray_vertexai' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "Set service account and grant the service account access to Vertex AI TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "a_C_BMVpzhug"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this default Service Account: 721521243942-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# @title Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lR2GDIkzp4af"
   },
   "outputs": [],
   "source": [
    "# ! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
    "#    --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
    "#    --role=\"roles/storage.admin\"\n",
    "\n",
    "# ! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
    "#    --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
    "#    --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/llmOps_vertexAI/training/ray/gemma\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/llmOps_vertexAI/training/ray/gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek1-iTbPjzdJ"
   },
   "source": [
    "### Set tutorial folder\n",
    "\n",
    "Set up the folder to use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BbfKRabXj3la"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path as path\n",
    "\n",
    "root_path = path.cwd()\n",
    "tutorial_path = root_path / \"tutorial\"\n",
    "data_path = tutorial_path / \"data\"\n",
    "src_path = tutorial_path / \"src\"\n",
    "experiments_path = tutorial_path / \"experiments\"\n",
    "models_path = tutorial_path / \"models\"\n",
    "build_path = tutorial_path / \"build\"\n",
    "tests_path = tutorial_path / \"tests\"\n",
    "\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "src_path.mkdir(parents=True, exist_ok=True)\n",
    "experiments_path.mkdir(parents=True, exist_ok=True)\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "build_path.mkdir(parents=True, exist_ok=True)\n",
    "tests_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ryiScCEapt"
   },
   "source": [
    "### Set a Ray cluster on Vertex AI\n",
    "\n",
    "Before running the code below, make sure to [set up](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/set-up) Ray on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_mlq_1NLEonF"
   },
   "outputs": [],
   "source": [
    "import vertex_ray\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from vertex_ray import NodeImages, Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dorIZFvjnGKL"
   },
   "source": [
    "#### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VOOgvRJoQ6Xj"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15PYjIyOcx1d"
   },
   "source": [
    "#### Build the custom cluster image\n",
    "\n",
    " It's necessary to utilize Ray Custom cluster image support since certain dependencies are required.\n",
    "\n",
    " To use a custom cluster image, the first step is to build the image. Below there are the steps to cover:\n",
    "\n",
    "*  Prepare the requirements file\n",
    "*  Create the Dockerfile for the custom image\n",
    "*  Create the Docker image repository\n",
    "*  Build the Ray cluster custom image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5vW6XpnkFeR"
   },
   "source": [
    "##### Prepare the requirements file\n",
    "\n",
    "Prepare the `requirements` file that includes the dependencies your Ray application needs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "83KUQQylbrJR"
   },
   "outputs": [],
   "source": [
    "requirements = \"\"\"\n",
    "ipython==8.22.2\n",
    "torch==2.2.1\n",
    "ray==2.10.0\n",
    "ray[data]==2.10.0\n",
    "ray[train]==2.10.0\n",
    "ray[tune]==2.10.0\n",
    "datasets==2.17.0\n",
    "transformers==4.38.1\n",
    "evaluate==0.4.1\n",
    "rouge-score==0.1.2\n",
    "nltk==3.8.1\n",
    "accelerate==0.27.1\n",
    "bitsandbytes==0.42.0\n",
    "peft==0.8.2\n",
    "trl==0.7.10\n",
    "# flash-attn==2.5.5\n",
    "pyarrow==15.0.2\n",
    "fsspec==2023.10.0\n",
    "gcsfs==2023.10.0\n",
    "etils==1.7.0\n",
    "importlib-resources==6.1.2\n",
    "\"\"\"\n",
    "\n",
    "with open(build_path / \"requirements.txt\", \"w\") as rfile:\n",
    "    rfile.write(requirements)\n",
    "rfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VHxHjCyKCyi"
   },
   "source": [
    "##### Create the Dockerfile\n",
    "\n",
    "Create the Dockerfile for the custom image by leveraging one of the prebuilt Ray on Vertex AI base images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iu7bgcdeXZIS"
   },
   "outputs": [],
   "source": [
    "CUSTOM_BASE_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-9.py310:latest\"  # @param [\"us-docker.pkg.dev/vertex-ai/training/ray-cpu.2-4.py310:latest\", \"us-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\", \"us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-4.py310:latest\", \"us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-9.py310:latest\", \"europe-docker.pkg.dev/vertex-ai/training/ray-cpu.2-4.py310:latest\", \"europe-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\", \"europe-docker.pkg.dev/vertex-ai/training/ray-gpu.2-4.py310:latest\", \"europe-docker.pkg.dev/vertex-ai/training/ray-gpu.2-9.py310:latest\", \"asia-docker.pkg.dev/vertex-ai/training/ray-cpu.2-4.py310:latest\", \"asia-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\", \"asia-docker.pkg.dev/vertex-ai/training/ray-gpu.2-4.py310:latest\", \"asia-docker.pkg.dev/vertex-ai/training/ray-gpu.2-9.py310:latest\"] {allow-input: true}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sRSzXRpxKB_Q"
   },
   "outputs": [],
   "source": [
    "dockerfile = f\"\"\"\n",
    "FROM {CUSTOM_BASE_IMAGE}\n",
    "\n",
    "# Install training libraries.\n",
    "ENV PIP_ROOT_USER_ACTION=ignore\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\"\"\"\n",
    "\n",
    "with open(build_path / \"Dockerfile\", \"w\") as image_file:\n",
    "    image_file.write(dockerfile)\n",
    "image_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTU6Ai5_RT1f"
   },
   "source": [
    "##### Create the Docker image repository\n",
    "\n",
    "To store the custom cluster image, create a Docker repository in the Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DUoNxKGIRiGb"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = f\"gemma-ray-vertexai\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Zh0iT2qVKBDi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "! gcloud artifacts repositories create {REPO_NAME} --repository-format=docker \\\n",
    "    --location={REGION} --description=\"Tutorial repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9lYOI15SFgh"
   },
   "source": [
    "##### Build the Ray cluster custom image\n",
    "\n",
    "Finally, build the Ray cluster custom image using Cloud Build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FR6BP7cgRLuD"
   },
   "outputs": [],
   "source": [
    "NODE_TRAIN_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/train\"\n",
    "BUILD_MACHINE_TYPE = \"E2_HIGHCPU_32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9VQKg4rMTLTa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 548 bytes before compression.\n",
      "Uploading tarball of [/home/jupyter/llmOps_vertexAI/training/ray/gemma/tutorial/build] to [gs://ai-hangsik_cloudbuild/source/1739200783.722574-eb6e3a5dfc2c44749de414651ed5b9a6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/ai-hangsik/locations/us-central1/builds/81b6d53f-ede1-408b-9bed-2e19e8818288].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/81b6d53f-ede1-408b-9bed-2e19e8818288?project=721521243942 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"81b6d53f-ede1-408b-9bed-2e19e8818288\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://ai-hangsik_cloudbuild/source/1739200783.722574-eb6e3a5dfc2c44749de414651ed5b9a6.tgz#1739200784014537\n",
      "Copying gs://ai-hangsik_cloudbuild/source/1739200783.722574-eb6e3a5dfc2c44749de414651ed5b9a6.tgz#1739200784014537...\n",
      "/ [1 files][  590.0 B/  590.0 B]                                                \n",
      "Operation completed over 1 objects/590.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.072kB\n",
      "Step 1/4 : FROM us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-9.py310:latest\n",
      "latest: Pulling from vertex-ai/training/ray-gpu.2-9.py310\n",
      "aece8493d397: Pulling fs layer\n",
      "5e3b7ee77381: Pulling fs layer\n",
      "5bd037f007fd: Pulling fs layer\n",
      "4cda774ad2ec: Pulling fs layer\n",
      "775f22adee62: Pulling fs layer\n",
      "263fc748118f: Pulling fs layer\n",
      "16c36d0187d0: Pulling fs layer\n",
      "e7a56570655c: Pulling fs layer\n",
      "507fc9045cba: Pulling fs layer\n",
      "23b7d8e07c16: Pulling fs layer\n",
      "922ac8fcb889: Pulling fs layer\n",
      "68075f2beca1: Pulling fs layer\n",
      "b02f5a289bcd: Pulling fs layer\n",
      "e6a258cf3414: Pulling fs layer\n",
      "ecabdea208c6: Pulling fs layer\n",
      "775f22adee62: Waiting\n",
      "4cda774ad2ec: Waiting\n",
      "ad5628ea6081: Pulling fs layer\n",
      "32edcaf961b8: Pulling fs layer\n",
      "e7a56570655c: Waiting\n",
      "a1c7e9c51fe1: Pulling fs layer\n",
      "507fc9045cba: Waiting\n",
      "53b52fcf17f0: Pulling fs layer\n",
      "23b7d8e07c16: Waiting\n",
      "0b8044a2892a: Pulling fs layer\n",
      "263fc748118f: Waiting\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b899b46e54a6: Pulling fs layer\n",
      "b02f5a289bcd: Waiting\n",
      "68075f2beca1: Waiting\n",
      "8afae4c5e732: Pulling fs layer\n",
      "922ac8fcb889: Waiting\n",
      "ecabdea208c6: Waiting\n",
      "7d762f733c3d: Pulling fs layer\n",
      "f5ca7f6ecf05: Pulling fs layer\n",
      "a1c7e9c51fe1: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "0b8044a2892a: Waiting\n",
      "5f4c84b86511: Pulling fs layer\n",
      "ad5628ea6081: Waiting\n",
      "32edcaf961b8: Waiting\n",
      "2192d8cb0a17: Pulling fs layer\n",
      "ba90734c688a: Pulling fs layer\n",
      "53b52fcf17f0: Waiting\n",
      "5114ffacf065: Pulling fs layer\n",
      "9e5132a01f67: Pulling fs layer\n",
      "fd0035ff7544: Pulling fs layer\n",
      "ff31d4a28830: Pulling fs layer\n",
      "d105d8934e70: Pulling fs layer\n",
      "5114ffacf065: Waiting\n",
      "8afae4c5e732: Waiting\n",
      "9e5132a01f67: Waiting\n",
      "fd0035ff7544: Waiting\n",
      "ba90734c688a: Waiting\n",
      "ff31d4a28830: Waiting\n",
      "f5ca7f6ecf05: Waiting\n",
      "2192d8cb0a17: Waiting\n",
      "5f4c84b86511: Waiting\n",
      "7d762f733c3d: Waiting\n",
      "d105d8934e70: Waiting\n",
      "5e3b7ee77381: Verifying Checksum\n",
      "5e3b7ee77381: Download complete\n",
      "4cda774ad2ec: Verifying Checksum\n",
      "4cda774ad2ec: Download complete\n",
      "775f22adee62: Verifying Checksum\n",
      "775f22adee62: Download complete\n",
      "aece8493d397: Verifying Checksum\n",
      "aece8493d397: Download complete\n",
      "16c36d0187d0: Verifying Checksum\n",
      "16c36d0187d0: Download complete\n",
      "5bd037f007fd: Download complete\n",
      "507fc9045cba: Verifying Checksum\n",
      "e7a56570655c: Download complete\n",
      "922ac8fcb889: Download complete\n",
      "aece8493d397: Pull complete\n",
      "5e3b7ee77381: Pull complete\n",
      "5bd037f007fd: Pull complete\n",
      "4cda774ad2ec: Pull complete\n",
      "775f22adee62: Pull complete\n",
      "68075f2beca1: Verifying Checksum\n",
      "68075f2beca1: Download complete\n",
      "b02f5a289bcd: Verifying Checksum\n",
      "b02f5a289bcd: Download complete\n",
      "e6a258cf3414: Verifying Checksum\n",
      "e6a258cf3414: Download complete\n",
      "263fc748118f: Verifying Checksum\n",
      "263fc748118f: Download complete\n",
      "ad5628ea6081: Verifying Checksum\n",
      "ad5628ea6081: Download complete\n",
      "ecabdea208c6: Verifying Checksum\n",
      "ecabdea208c6: Download complete\n",
      "a1c7e9c51fe1: Verifying Checksum\n",
      "a1c7e9c51fe1: Download complete\n",
      "32edcaf961b8: Verifying Checksum\n",
      "32edcaf961b8: Download complete\n",
      "53b52fcf17f0: Verifying Checksum\n",
      "53b52fcf17f0: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "b899b46e54a6: Verifying Checksum\n",
      "b899b46e54a6: Download complete\n",
      "8afae4c5e732: Verifying Checksum\n",
      "8afae4c5e732: Download complete\n",
      "7d762f733c3d: Verifying Checksum\n",
      "7d762f733c3d: Download complete\n",
      "0b8044a2892a: Verifying Checksum\n",
      "0b8044a2892a: Download complete\n",
      "5f4c84b86511: Verifying Checksum\n",
      "5f4c84b86511: Download complete\n",
      "2192d8cb0a17: Verifying Checksum\n",
      "2192d8cb0a17: Download complete\n",
      "ba90734c688a: Verifying Checksum\n",
      "ba90734c688a: Download complete\n",
      "5114ffacf065: Verifying Checksum\n",
      "5114ffacf065: Download complete\n",
      "9e5132a01f67: Verifying Checksum\n",
      "9e5132a01f67: Download complete\n",
      "f5ca7f6ecf05: Verifying Checksum\n",
      "f5ca7f6ecf05: Download complete\n",
      "ff31d4a28830: Verifying Checksum\n",
      "ff31d4a28830: Download complete\n",
      "fd0035ff7544: Download complete\n",
      "d105d8934e70: Verifying Checksum\n",
      "d105d8934e70: Download complete\n",
      "23b7d8e07c16: Verifying Checksum\n",
      "23b7d8e07c16: Download complete\n",
      "263fc748118f: Pull complete\n",
      "16c36d0187d0: Pull complete\n",
      "e7a56570655c: Pull complete\n",
      "507fc9045cba: Pull complete\n",
      "23b7d8e07c16: Pull complete\n",
      "922ac8fcb889: Pull complete\n",
      "68075f2beca1: Pull complete\n",
      "b02f5a289bcd: Pull complete\n",
      "e6a258cf3414: Pull complete\n",
      "ecabdea208c6: Pull complete\n",
      "ad5628ea6081: Pull complete\n",
      "32edcaf961b8: Pull complete\n",
      "a1c7e9c51fe1: Pull complete\n",
      "53b52fcf17f0: Pull complete\n",
      "0b8044a2892a: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "b899b46e54a6: Pull complete\n",
      "8afae4c5e732: Pull complete\n",
      "7d762f733c3d: Pull complete\n",
      "f5ca7f6ecf05: Pull complete\n",
      "5f4c84b86511: Pull complete\n",
      "2192d8cb0a17: Pull complete\n",
      "ba90734c688a: Pull complete\n",
      "5114ffacf065: Pull complete\n",
      "9e5132a01f67: Pull complete\n",
      "fd0035ff7544: Pull complete\n",
      "ff31d4a28830: Pull complete\n",
      "d105d8934e70: Pull complete\n",
      "Digest: sha256:c3f9f7dd0f87d9fde70753f8fd3a51d70a0389394d26a86fc866947deb041a49\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-9.py310:latest\n",
      " ---> 28c7440b75c3\n",
      "Step 2/4 : ENV PIP_ROOT_USER_ACTION=ignore\n",
      " ---> Running in df0c485db779\n",
      "Removing intermediate container df0c485db779\n",
      " ---> 14284cfdbb45\n",
      "Step 3/4 : COPY requirements.txt .\n",
      " ---> 5a0bf4edab36\n",
      "Step 4/4 : RUN pip install -r requirements.txt\n",
      " ---> Running in 29faf90b7023\n",
      "Collecting ipython==8.22.2 (from -r requirements.txt (line 2))\n",
      "  Downloading ipython-8.22.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting torch==2.2.1 (from -r requirements.txt (line 3))\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting ray==2.10.0 (from -r requirements.txt (line 4))\n",
      "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting datasets==2.17.0 (from -r requirements.txt (line 8))\n",
      "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers==4.38.1 (from -r requirements.txt (line 9))\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "Collecting evaluate==0.4.1 (from -r requirements.txt (line 10))\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting rouge-score==0.1.2 (from -r requirements.txt (line 11))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting nltk==3.8.1 (from -r requirements.txt (line 12))\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting accelerate==0.27.1 (from -r requirements.txt (line 13))\n",
      "  Downloading accelerate-0.27.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes==0.42.0 (from -r requirements.txt (line 14))\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting peft==0.8.2 (from -r requirements.txt (line 15))\n",
      "  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting trl==0.7.10 (from -r requirements.txt (line 16))\n",
      "  Downloading trl-0.7.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyarrow==15.0.2 (from -r requirements.txt (line 18))\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting fsspec==2023.10.0 (from -r requirements.txt (line 19))\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting gcsfs==2023.10.0 (from -r requirements.txt (line 20))\n",
      "  Downloading gcsfs-2023.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting etils==1.7.0 (from -r requirements.txt (line 21))\n",
      "  Downloading etils-1.7.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting importlib-resources==6.1.2 (from -r requirements.txt (line 22))\n",
      "  Downloading importlib_resources-6.1.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython==8.22.2->-r requirements.txt (line 2)) (5.1.1)\n",
      "Collecting jedi>=0.16 (from ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting matplotlib-inline (from ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading prompt_toolkit-3.0.50-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython==8.22.2->-r requirements.txt (line 2)) (2.18.0)\n",
      "Collecting stack-data (from ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting traitlets>=5.13.0 (from ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython==8.22.2->-r requirements.txt (line 2)) (1.2.2)\n",
      "Collecting pexpect>4.3 (from ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.1->-r requirements.txt (line 3)) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.1->-r requirements.txt (line 3)) (4.12.2)\n",
      "Collecting sympy (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.1->-r requirements.txt (line 3)) (3.3)\n",
      "Collecting jinja2 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (24.1)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (3.20.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0->-r requirements.txt (line 8)) (2.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets==2.17.0->-r requirements.txt (line 8))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.17.0->-r requirements.txt (line 8))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0->-r requirements.txt (line 8)) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0->-r requirements.txt (line 8)) (4.66.5)\n",
      "Collecting xxhash (from datasets==2.17.0->-r requirements.txt (line 8))\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.17.0->-r requirements.txt (line 8))\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0->-r requirements.txt (line 8)) (3.10.1)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets==2.17.0->-r requirements.txt (line 8))\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.38.1->-r requirements.txt (line 9))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.1->-r requirements.txt (line 9))\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.1->-r requirements.txt (line 9))\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting responses<0.19 (from evaluate==0.4.1->-r requirements.txt (line 10))\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score==0.1.2->-r requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score==0.1.2->-r requirements.txt (line 11)) (1.16.0)\n",
      "Collecting joblib (from nltk==3.8.1->-r requirements.txt (line 12))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.1->-r requirements.txt (line 13)) (6.0.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r requirements.txt (line 14)) (1.14.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.7.10->-r requirements.txt (line 16))\n",
      "  Downloading tyro-0.9.13-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting numpy>=1.17 (from datasets==2.17.0->-r requirements.txt (line 8))\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0->-r requirements.txt (line 20)) (2.32.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0->-r requirements.txt (line 20)) (1.2.1)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0->-r requirements.txt (line 20)) (2.18.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.10/site-packages (from ray[train]==2.10.0->-r requirements.txt (line 6)) (2.6.2.2)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0->-r requirements.txt (line 8)) (2.3.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0->-r requirements.txt (line 8)) (24.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0->-r requirements.txt (line 8)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0->-r requirements.txt (line 8)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0->-r requirements.txt (line 8)) (4.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0->-r requirements.txt (line 20)) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0->-r requirements.txt (line 20)) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0->-r requirements.txt (line 20)) (4.9)\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0->-r requirements.txt (line 8)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0->-r requirements.txt (line 8)) (2024.1)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython==8.22.2->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0->-r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0->-r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0->-r requirements.txt (line 4)) (2024.7.4)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.10->-r requirements.txt (line 16)) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.10->-r requirements.txt (line 16)) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.10->-r requirements.txt (line 16))\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.7.10->-r requirements.txt (line 16))\n",
      "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs==2023.10.0->-r requirements.txt (line 20)) (2.0.0)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage->gcsfs==2023.10.0->-r requirements.txt (line 20))\n",
      "  Downloading google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0->-r requirements.txt (line 20)) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0->-r requirements.txt (line 20)) (2.7.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0->-r requirements.txt (line 20)) (1.5.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0->-r requirements.txt (line 4)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0->-r requirements.txt (line 4)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0->-r requirements.txt (line 4)) (0.19.1)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.17.0->-r requirements.txt (line 8))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting executing>=1.2.0 (from stack-data->ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack-data->ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pure-eval (from stack-data->ipython==8.22.2->-r requirements.txt (line 2))\n",
      "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.1->-r requirements.txt (line 3))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2023.10.0->-r requirements.txt (line 20)) (1.63.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2023.10.0->-r requirements.txt (line 20)) (1.24.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2023.10.0->-r requirements.txt (line 20)) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2023.10.0->-r requirements.txt (line 20)) (3.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.10->-r requirements.txt (line 16)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.10->-r requirements.txt (line 16)) (0.1.2)\n",
      "Downloading ipython-8.22.2-py3-none-any.whl (811 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 812.0/812.0 kB 26.7 MB/s eta 0:00:00\n",
      "Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.5/755.5 MB 62.8 MB/s eta 0:00:00\n",
      "Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.1/65.1 MB 145.5 MB/s eta 0:00:00\n",
      "Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.6/536.6 kB 35.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 148.4 MB/s eta 0:00:00\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 97.6 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.27.1-py3-none-any.whl (279 kB)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.0/105.0 MB 145.7 MB/s eta 0:00:00\n",
      "Downloading peft-0.8.2-py3-none-any.whl (183 kB)\n",
      "Downloading trl-0.7.10-py3-none-any.whl (150 kB)\n",
      "Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.3/38.3 MB 133.1 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading gcsfs-2023.10.0-py2.py3-none-any.whl (33 kB)\n",
      "Downloading etils-1.7.0-py3-none-any.whl (152 kB)\n",
      "Downloading importlib_resources-6.1.2-py3-none-any.whl (34 kB)\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 81.5 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 117.5 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 125.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 53.3 MB/s eta 0:00:00\n",
      "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 47.1 MB/s eta 0:00:00\n",
      "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 76.1 MB/s eta 0:00:00\n",
      "Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 82.2 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 68.1 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 63.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 66.2 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 MB 82.3 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 56.9 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 90.3 MB/s eta 0:00:00\n",
      "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Downloading prompt_toolkit-3.0.50-py3-none-any.whl (387 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.7/781.7 kB 48.3 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 75.6 MB/s eta 0:00:00\n",
      "Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "Downloading tyro-0.9.13-py3-none-any.whl (115 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 81.4 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading google_api_core-2.24.1-py3-none-any.whl (160 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 32.3 MB/s eta 0:00:00\n",
      "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
      "Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.2/39.2 MB 87.1 MB/s eta 0:00:00\n",
      "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=864654cb0dbcc0be1f1d15301e466608e243c609f551e10cf9126d10233364be\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: pure-eval, ptyprocess, mpmath, xxhash, typeguard, triton, traitlets, sympy, shtab, safetensors, regex, pyarrow-hotfix, prompt-toolkit, pexpect, parso, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, MarkupSafe, joblib, importlib-resources, fsspec, executing, etils, dill, asttokens, stack-data, responses, pyarrow, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, matplotlib-inline, jinja2, jedi, huggingface-hub, tyro, tokenizers, rouge-score, nvidia-cusolver-cu12, ipython, google-api-core, bitsandbytes, transformers, torch, ray, datasets, evaluate, accelerate, trl, peft, gcsfs\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 17.0.0\n",
      "    Uninstalling pyarrow-17.0.0:\n",
      "      Successfully uninstalled pyarrow-17.0.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.34.1\n",
      "    Uninstalling google-api-core-1.34.1:\n",
      "      Successfully uninstalled google-api-core-1.34.1\n",
      "  Attempting uninstall: ray\n",
      "    Found existing installation: ray 2.9.3\n",
      "    Uninstalling ray-2.9.3:\n",
      "      Successfully uninstalled ray-2.9.3\n",
      "  Attempting uninstall: gcsfs\n",
      "    Found existing installation: gcsfs 2024.6.1\n",
      "    Uninstalling gcsfs-2024.6.1:\n",
      "      Successfully uninstalled gcsfs-2024.6.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.24.1 which is incompatible.\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 accelerate-0.27.1 asttokens-3.0.0 bitsandbytes-0.42.0 datasets-2.17.0 dill-0.3.8 etils-1.7.0 evaluate-0.4.1 executing-2.2.0 fsspec-2023.10.0 gcsfs-2023.10.0 google-api-core-2.24.1 huggingface-hub-0.28.1 importlib-resources-6.1.2 ipython-8.22.2 jedi-0.19.2 jinja2-3.1.5 joblib-1.4.2 matplotlib-inline-0.1.7 mpmath-1.3.0 multiprocess-0.70.16 nltk-3.8.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.8.61 nvidia-nvtx-cu12-12.1.105 parso-0.8.4 peft-0.8.2 pexpect-4.9.0 prompt-toolkit-3.0.50 ptyprocess-0.7.0 pure-eval-0.2.3 pyarrow-15.0.2 pyarrow-hotfix-0.6 ray-2.10.0 regex-2024.11.6 responses-0.18.0 rouge-score-0.1.2 safetensors-0.5.2 shtab-1.7.1 stack-data-0.6.3 sympy-1.13.3 tokenizers-0.15.2 torch-2.2.1 traitlets-5.14.3 transformers-4.38.1 triton-2.2.0 trl-0.7.10 typeguard-4.4.1 tyro-0.9.13 xxhash-3.5.0\n",
      "Removing intermediate container 29faf90b7023\n",
      " ---> a45452e2be28\n",
      "Successfully built a45452e2be28\n",
      "Successfully tagged us-central1-docker.pkg.dev/ai-hangsik/gemma-ray-vertexai/train:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/ai-hangsik/gemma-ray-vertexai/train\n",
      "The push refers to repository [us-central1-docker.pkg.dev/ai-hangsik/gemma-ray-vertexai/train]\n",
      "3f505c3440e9: Preparing\n",
      "fc574bc84383: Preparing\n",
      "020a584004e6: Preparing\n",
      "cf4d7fae7358: Preparing\n",
      "dd0b3fdaae58: Preparing\n",
      "d31489bed7fb: Preparing\n",
      "249c7a1de581: Preparing\n",
      "0c8973bce2ed: Preparing\n",
      "f82497ac11cb: Preparing\n",
      "7896e64484b1: Preparing\n",
      "7e823e50182a: Preparing\n",
      "41af5164fc6f: Preparing\n",
      "ec22db150e0c: Preparing\n",
      "ec22db150e0c: Preparing\n",
      "58c4b4f5982d: Preparing\n",
      "249c7a1de581: Waiting\n",
      "5f70bf18a086: Preparing\n",
      "ff7858c28302: Preparing\n",
      "7896e64484b1: Waiting\n",
      "d0c8d57378b1: Preparing\n",
      "6acd2888117a: Preparing\n",
      "9cdc8a6af373: Preparing\n",
      "ce8f9fe388dd: Preparing\n",
      "7e823e50182a: Waiting\n",
      "f6d4f1bbf4dd: Preparing\n",
      "dc8ee65a488d: Preparing\n",
      "41af5164fc6f: Waiting\n",
      "ad53718ea3e2: Preparing\n",
      "d31489bed7fb: Waiting\n",
      "ec22db150e0c: Waiting\n",
      "383e6312d4f9: Preparing\n",
      "58c4b4f5982d: Waiting\n",
      "64758552f6fa: Preparing\n",
      "23d753990c8d: Preparing\n",
      "5f70bf18a086: Waiting\n",
      "345cfa465206: Preparing\n",
      "ff7858c28302: Waiting\n",
      "dcb0f55f81ad: Preparing\n",
      "d0c8d57378b1: Waiting\n",
      "399d155a03b0: Preparing\n",
      "bc352a27a0e4: Preparing\n",
      "6acd2888117a: Waiting\n",
      "9cdc8a6af373: Waiting\n",
      "498bbcc60d01: Preparing\n",
      "c0e21dcee623: Preparing\n",
      "0c8973bce2ed: Waiting\n",
      "d6b19a46b795: Preparing\n",
      "ce8f9fe388dd: Waiting\n",
      "e6c05e83c163: Preparing\n",
      "ad53718ea3e2: Waiting\n",
      "256d88da4185: Preparing\n",
      "f6d4f1bbf4dd: Waiting\n",
      "383e6312d4f9: Waiting\n",
      "23d753990c8d: Waiting\n",
      "dc8ee65a488d: Waiting\n",
      "345cfa465206: Waiting\n",
      "f82497ac11cb: Waiting\n",
      "bc352a27a0e4: Waiting\n",
      "e6c05e83c163: Waiting\n",
      "d6b19a46b795: Waiting\n",
      "c0e21dcee623: Waiting\n",
      "256d88da4185: Waiting\n",
      "498bbcc60d01: Waiting\n",
      "cf4d7fae7358: Layer already exists\n",
      "dd0b3fdaae58: Layer already exists\n",
      "020a584004e6: Layer already exists\n",
      "d31489bed7fb: Layer already exists\n",
      "0c8973bce2ed: Layer already exists\n",
      "249c7a1de581: Layer already exists\n",
      "7896e64484b1: Layer already exists\n",
      "f82497ac11cb: Layer already exists\n",
      "7e823e50182a: Layer already exists\n",
      "ec22db150e0c: Layer already exists\n",
      "58c4b4f5982d: Layer already exists\n",
      "41af5164fc6f: Layer already exists\n",
      "ff7858c28302: Layer already exists\n",
      "fc574bc84383: Pushed\n",
      "5f70bf18a086: Layer already exists\n",
      "d0c8d57378b1: Layer already exists\n",
      "6acd2888117a: Layer already exists\n",
      "9cdc8a6af373: Layer already exists\n",
      "ce8f9fe388dd: Layer already exists\n",
      "f6d4f1bbf4dd: Layer already exists\n",
      "ad53718ea3e2: Layer already exists\n",
      "383e6312d4f9: Layer already exists\n",
      "dc8ee65a488d: Layer already exists\n",
      "64758552f6fa: Layer already exists\n",
      "23d753990c8d: Layer already exists\n",
      "345cfa465206: Layer already exists\n",
      "dcb0f55f81ad: Layer already exists\n",
      "bc352a27a0e4: Layer already exists\n",
      "399d155a03b0: Layer already exists\n",
      "c0e21dcee623: Layer already exists\n",
      "498bbcc60d01: Layer already exists\n",
      "d6b19a46b795: Layer already exists\n",
      "e6c05e83c163: Layer already exists\n",
      "256d88da4185: Layer already exists\n",
      "3f505c3440e9: Pushed\n",
      "latest: digest: sha256:3ce2b88a8f82e2b2c9b52a9a685cbc1558bdc3256ebffd744a866a1bbbbab9d1 size: 7865\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                                                    STATUS\n",
      "81b6d53f-ede1-408b-9bed-2e19e8818288  2025-02-10T15:19:44+00:00  12M11S    gs://ai-hangsik_cloudbuild/source/1739200783.722574-eb6e3a5dfc2c44749de414651ed5b9a6.tgz  us-central1-docker.pkg.dev/ai-hangsik/gemma-ray-vertexai/train (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --region={REGION} --tag={NODE_TRAIN_IMAGE} \\\n",
    "    --machine-type={BUILD_MACHINE_TYPE} --timeout=3600 {build_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_O1xUMt7Z6r0"
   },
   "source": [
    "#### Create the Ray cluster\n",
    "\n",
    "With the custom image, create the Ray cluster using the custom image via Ray on Vertex AI SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eZkHOH3v2i1p"
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = f\"gemma-ray-vertexai-cluster\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd2TmEdyVLo1"
   },
   "source": [
    "###### Set the Ray cluster configuration\n",
    "\n",
    "Use the Vertex AI Python SDK for Ray on Vertex AI to set the cluster configuration.\n",
    "\n",
    "To know more about the cluster configuration, see the [documentation](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster#ray-on-vertex-ai-sdk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "m0BBynZrFn7z"
   },
   "outputs": [],
   "source": [
    "HEAD_NODE_MACHINE_TYPE = \"n1-standard-16\"  # @param {type:\"string\"}\n",
    "HEAD_NODE_COUNT = 1  # @param {type:\"integer\"}\n",
    "\n",
    "WORKER_NODE_MACHINE_TYPE = \"a2-highgpu-1g\"  # @param {type:\"string\"}\n",
    "WORKER_NODE_COUNT = 1  # @param {type:\"integer\"}\n",
    "WORKER_ACCELERATION_TYPE = \"NVIDIA_TESLA_A100\"  # @param {type:\"string\"}\n",
    "WORKER_ACCELERATION_COUNT = 1  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ppNbbhgm3_GE"
   },
   "outputs": [],
   "source": [
    "HEAD_NODE_TYPE = Resources(\n",
    "    machine_type=HEAD_NODE_MACHINE_TYPE,\n",
    "    node_count=HEAD_NODE_COUNT,\n",
    ")\n",
    "\n",
    "WORKER_NODE_TYPES = [\n",
    "    Resources(\n",
    "        machine_type=WORKER_NODE_MACHINE_TYPE,\n",
    "        node_count=WORKER_NODE_COUNT,\n",
    "        accelerator_type=WORKER_ACCELERATION_TYPE,\n",
    "        accelerator_count=WORKER_ACCELERATION_COUNT,\n",
    "    )\n",
    "]\n",
    "\n",
    "CUSTOM_IMAGES = NodeImages(\n",
    "    head=NODE_TRAIN_IMAGE,\n",
    "    worker=NODE_TRAIN_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfkpGqRjksaW"
   },
   "source": [
    "##### Create the Ray cluster\n",
    "\n",
    "Create the Ray cluster with the predefined custom configuration. Creating a cluster can take several minutes, depending on its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "J-g6kLwqUj5n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ray on Vertex AI]: Cluster State = State.PROVISIONING\n",
      "Waiting for cluster provisioning; attempt 1; sleeping for 0:02:30 seconds\n",
      "[Ray on Vertex AI]: Cluster State = State.PROVISIONING\n",
      "Waiting for cluster provisioning; attempt 2; sleeping for 0:01:54.750000 seconds\n",
      "[Ray on Vertex AI]: Cluster State = State.PROVISIONING\n",
      "Waiting for cluster provisioning; attempt 3; sleeping for 0:01:27.783750 seconds\n",
      "[Ray on Vertex AI]: Cluster State = State.PROVISIONING\n",
      "Waiting for cluster provisioning; attempt 4; sleeping for 0:01:07.154569 seconds\n",
      "[Ray on Vertex AI]: Cluster State = State.PROVISIONING\n",
      "Waiting for cluster provisioning; attempt 5; sleeping for 0:00:51.373245 seconds\n",
      "[Ray on Vertex AI]: Cluster State = State.PROVISIONING\n",
      "Waiting for cluster provisioning; attempt 6; sleeping for 0:00:39.300532 seconds\n",
      "[Ray on Vertex AI]: Cluster State = State.PROVISIONING\n",
      "Waiting for cluster provisioning; attempt 7; sleeping for 0:00:30.064907 seconds\n",
      "[Ray on Vertex AI]: Cluster State = State.PROVISIONING\n",
      "Waiting for cluster provisioning; attempt 8; sleeping for 0:00:30.064907 seconds\n",
      "[Ray on Vertex AI]: Cluster State = State.RUNNING\n"
     ]
    }
   ],
   "source": [
    "ray_cluster_name = vertex_ray.create_ray_cluster(\n",
    "    head_node_type=HEAD_NODE_TYPE,\n",
    "    worker_node_types=WORKER_NODE_TYPES,\n",
    "    custom_images=CUSTOM_IMAGES,\n",
    "    cluster_name=CLUSTER_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmBlsbHAc2uO"
   },
   "source": [
    "##### Get the Ray cluster\n",
    "\n",
    "Use the Ray on Vertex AI SDK for Python to get the Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9UzG2WyXbZJi"
   },
   "outputs": [],
   "source": [
    "ray_clusters = vertex_ray.list_ray_clusters()\n",
    "ray_cluster_resource_name = ray_clusters[-1].cluster_resource_name\n",
    "ray_cluster = vertex_ray.get_ray_cluster(ray_cluster_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "g7ZKdv5-GCWr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray cluster on Vertex AI: projects/721521243942/locations/us-central1/persistentResources/ray-cluster-20250209235548\n"
     ]
    }
   ],
   "source": [
    "print(\"Ray cluster on Vertex AI:\", ray_cluster_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnvJitLink.so.12: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Ray - Training\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m epath\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:237\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    236\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: libnvJitLink.so.12: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# Ray - Training\n",
    "import ray\n",
    "import torch\n",
    "import transformers\n",
    "from etils import epath\n",
    "from google.cloud import storage\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "from ray.job_submission import JobStatus, JobSubmissionClient\n",
    "\n",
    "# Ray - Batch Serving\n",
    "from ray.tune import ExperimentAnalysis\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "h9_UttTcNGYN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRay version: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mray\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ray' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Ray version: \", ray.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFgb-sZbBi8i"
   },
   "source": [
    "### Set variables\n",
    "\n",
    "Initiate some tutorial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zykxFjqUB9jt"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m HF_TOKEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_AuNJZyoCEOPUpSHweijndsRtQGeqaIHMfH\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# @param {type:\"string\"}\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m EXPERIMENTS_FOLDER_URI \u001b[38;5;241m=\u001b[39m \u001b[43mepath\u001b[49m\u001b[38;5;241m.\u001b[39mPath(BUCKET_URI) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m TENSORBOARD_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrov-xsum-gemma-tb-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTIMESTAMP\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Serving\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epath' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "HF_TOKEN = \"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\"  # @param {type:\"string\"}\n",
    "EXPERIMENTS_FOLDER_URI = epath.Path(BUCKET_URI) / \"experiments\"\n",
    "TENSORBOARD_NAME = f\"rov-xsum-gemma-tb-{TIMESTAMP}\"\n",
    "\n",
    "# Serving\n",
    "MODELS_PATH = epath.Path(BUCKET_URI) / \"models\"\n",
    "PREDICTIONS_FOLDER_URI = epath.Path(BUCKET_URI) / \"predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYQNboaOBk6B"
   },
   "source": [
    "### Define helpers\n",
    "\n",
    "Define an helper function to monitor the status of Ray job using Ray Dashboard API in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrvG6VDIm9hG"
   },
   "outputs": [],
   "source": [
    "def monitor_job(client, job_id):\n",
    "    \"\"\"Monitors the status of Ray job using Ray Dashboard API\"\"\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=f\"%(asctime)s.%(msecs)03d %(levelname)s {job_id} -- %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        force=True,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        job_status = client.get_job_status(job_id)\n",
    "\n",
    "        if job_status == JobStatus.SUCCEEDED:\n",
    "            logging.info(\"Job succeeded!\")\n",
    "            break\n",
    "\n",
    "        elif job_status == JobStatus.FAILED:\n",
    "            logging.info(\"Job failed!\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            logging.info(\"Job is running...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    return job_status\n",
    "\n",
    "\n",
    "def read_json_files(bucket_name, prefix=None):\n",
    "    \"\"\"Reads JSON files from a cloud storage bucket and returns a Pandas DataFrame\"\"\"\n",
    "\n",
    "    # Set up storage client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\".json\"):\n",
    "            file_bytes = blob.download_as_bytes()\n",
    "            file_string = file_bytes.decode(\"utf-8\")\n",
    "            with io.StringIO(file_string) as json_file:\n",
    "                df = pd.read_json(json_file, lines=True)\n",
    "            dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkAwZBj71ipc"
   },
   "source": [
    "### Libraries settings\n",
    "\n",
    "Initiate some libraries settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3hT5TUM1lGc"
   },
   "outputs": [],
   "source": [
    "login(token=HF_TOKEN)\n",
    "datasets.disable_progress_bar()\n",
    "transformers.set_seed(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assh71yp4G_O"
   },
   "source": [
    "### Create a Vertex AI TensorBoard instance\n",
    "\n",
    "Create a Vertex AI TensorBoard instance for tracking and monitoring your tuning jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKyJ1vREZRIY"
   },
   "outputs": [],
   "source": [
    "tensorboard = vertex_ai.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_NAME, project=PROJECT_ID, location=REGION\n",
    ")\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET_URI,\n",
    "    experiment_tensorboard=tensorboard,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BH0TlP3PtTB"
   },
   "source": [
    "## Fine-Tune Gemma with Ray Train\n",
    "\n",
    "In this tutorial, you fine-tune Gemma 2B (`gemma-2b-it`) for summarizing newspaper articles using HuggingFace Transformer on Ray on Vertex AI. In an effort to make this notebook easily reproducible, you write a simple Python `trainer.py` script and submit it to the Ray cluster on Vertex AI using the Ray Jobs API via the public Ray Dashboard.\n",
    "\n",
    "As mentioned at the beginning, **consider this option for experimentation only.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du1IAdWos0AF"
   },
   "source": [
    "### Initialize the Ray package\n",
    "\n",
    "Create an `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWrX2WLUs4um"
   },
   "outputs": [],
   "source": [
    "with open(src_path / \"__init__.py\", \"a\") as init_file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVL71KFDstVR"
   },
   "source": [
    "### Prepare the train script\n",
    "\n",
    "Create the `src/train.py` file which is the Python script for initializing Gemma fine-tuning using HuggingFace TRL library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNQXWOHwtIDi"
   },
   "outputs": [],
   "source": [
    "train_script = '''\n",
    "# training libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Seq2SeqTrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "import ray\n",
    "import ray.train.huggingface.transformers\n",
    "\n",
    "def train_func(config):\n",
    "    # Helpers\n",
    "    def formatting_func(example):\n",
    "        \"\"\"Helper function for formatting data for instruction tuning according to Gemma documentation.\"\"\"\n",
    "        output_texts = []\n",
    "        for i in range(len(example)):\n",
    "          messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"Summarize the following ARTICLE in one sentence.\\\\n###ARTICLE: {example['document'][i]}\"},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": f\"{example['summary'][i]}<eos>\"} # Make minor gemma fixes #2029\n",
    "             ]\n",
    "          output_texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))\n",
    "        return output_texts\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        \"\"\"Helper function for computing metrics\"\"\"\n",
    "        preds, labels = eval_preds\n",
    "        preds = preds[0]\n",
    "\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        metrics = rouge.compute(predictions=decoded_preds,\n",
    "                                references=decoded_labels,\n",
    "                                rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
    "                                use_aggregator=True, use_stemmer=True)\n",
    "        metrics = {k: round(v * 100, 4) for k, v in metrics.items()}\n",
    "        return metrics\n",
    "\n",
    "    def preprocess_logits_for_metrics(logits, labels):\n",
    "        \"\"\"Helper function for logits preprocessing for metrics\"\"\"\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        return preds, labels\n",
    "\n",
    "    # Setting training\n",
    "    login(token=os.environ['HF_TOKEN'], add_to_git_credential=True)\n",
    "    transformers.set_seed(8)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset_id = \"xsum\"\n",
    "    dataset = datasets.load_dataset(dataset_id, trust_remote_code=True)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    # Preprocess dataset\n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "    # Prepare model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 device_map={'': torch.cuda.current_device()},\n",
    "                                                 torch_dtype=torch.bfloat16,\n",
    "                                                 # attn_implementation=\"flash_attention_2\"\n",
    "                                                 )\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=\"all-linear\",\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    # model.gradient_checkpointing_enable()\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"checkpoints\",\n",
    "        per_device_train_batch_size=config.get(\"per_device_train_batch_size\"),\n",
    "        per_device_eval_batch_size=config.get(\"per_device_eval_batch_size\"),\n",
    "        gradient_accumulation_steps=config.get(\"gradient_accumulation_steps\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        max_steps=config.get(\"max_steps\"),\n",
    "        save_steps=config.get(\"save_steps\"),\n",
    "        logging_steps=config.get(\"logging_steps\"),\n",
    "        learning_rate=config.get(\"learning_rate\"),\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        bf16=False,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        gradient_checkpointing=True,\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=False\n",
    "    )\n",
    "\n",
    "    max_seq_length = 512\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        max_seq_length=max_seq_length,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        peft_config=lora_config,\n",
    "        formatting_func=formatting_func\n",
    "    )\n",
    "    # model.config.use_cache = False\n",
    "\n",
    "    callback = ray.train.huggingface.transformers.RayTrainReportCallback()\n",
    "    trainer.add_callback(callback)\n",
    "    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n",
    "    trainer.train()\n",
    "'''\n",
    "\n",
    "with open(src_path / \"train.py\", \"w\") as f:\n",
    "    f.write(train_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pznqJKCpEcL3"
   },
   "source": [
    "### Prepare the distributed training script\n",
    "\n",
    "Create `src/trainer.py` file which is the Python script for executing the Ray distributed training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCFfhM7RP4RI"
   },
   "outputs": [],
   "source": [
    "trainer_script = \"\"\"\n",
    "# libraries\n",
    "import argparse\n",
    "\n",
    "# training libraries\n",
    "from train import train_func\n",
    "\n",
    "# ray libraries\n",
    "import ray\n",
    "import ray.train.huggingface.transformers\n",
    "from ray.train import ScalingConfig, RunConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "# helpers\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Supervised tuning Gemma on Ray on Vertex AI')\n",
    "\n",
    "    # some gemma parameters\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=1, help=\"train batch size\")\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"eval batch size\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4, help=\"gradient accumulation steps\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"learning rate\")\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=100, help=\"max steps\")\n",
    "    parser.add_argument(\"--save_steps\", type=int, default=10, help=\"save steps\")\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=10, help=\"logging steps\")\n",
    "\n",
    "    # ray parameters\n",
    "    parser.add_argument('--num-workers', dest='num_workers', type=int, default=1, help='Number of workers')\n",
    "    parser.add_argument('--use-gpu', dest='use_gpu', action='store_true', default=False, help='Use GPU')\n",
    "    parser.add_argument('--experiment-name', dest='experiment_name', type=str, default='gemma-on-rov', help='Experiment name')\n",
    "    parser.add_argument('--logging-dir', dest='logging_dir', type=str, help='Logging directory')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = get_args()\n",
    "    config = vars(args)\n",
    "\n",
    "    # initialize ray session\n",
    "    ray.shutdown()\n",
    "    ray.init()\n",
    "\n",
    "    # training config\n",
    "    train_loop_config = {\n",
    "        \"per_device_train_batch_size\": config['train_batch_size'],\n",
    "        \"per_device_eval_batch_size\": config['eval_batch_size'],\n",
    "        \"gradient_accumulation_steps\": config['gradient_accumulation_steps'],\n",
    "        \"learning_rate\": config['learning_rate'],\n",
    "        \"max_steps\": config['max_steps'],\n",
    "        \"save_steps\": config['save_steps'],\n",
    "        \"logging_steps\": config['logging_steps'],\n",
    "    }\n",
    "    scaling_config = ScalingConfig(num_workers=config['num_workers'], use_gpu=config['use_gpu'])\n",
    "    run_config = RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=5,\n",
    "                          checkpoint_score_attribute=\"loss\",\n",
    "                          checkpoint_score_order=\"min\"),\n",
    "                           storage_path=config['logging_dir'],\n",
    "                           name=config['experiment_name'])\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func,\n",
    "        train_loop_config=train_loop_config,\n",
    "        run_config=run_config,\n",
    "        scaling_config=scaling_config\n",
    "    )\n",
    "    # train\n",
    "    result = trainer.fit()\n",
    "\n",
    "    ray.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(src_path / \"trainer.py\", \"w\") as f:\n",
    "    f.write(trainer_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYcmfEvZ3C1i"
   },
   "source": [
    "### Submit a Ray job using the Ray Jobs API\n",
    "\n",
    "Submit the script to the Ray cluster on Vertex AI using the Ray Jobs API with  the public Ray dashboard address.\n",
    "\n",
    "Initiate the client to submit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FljDHRQ63EP4"
   },
   "outputs": [],
   "source": [
    "client = JobSubmissionClient(\n",
    "    address=\"vertex_ray://{}\".format(ray_cluster.dashboard_address)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDHIlGlQJ2oi"
   },
   "source": [
    "Set some job configuration including experiment name, job id, training entrypoint and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxkqJ9vntz7C"
   },
   "outputs": [],
   "source": [
    "train_id = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=3))\n",
    "train_experiment_name = f\"rov-dialog-gemma-tune-{train_id}\"\n",
    "train_submission_id = f\"ray-job-{train_id}\"\n",
    "train_entrypoint = f\"python3 trainer.py --experiment-name={train_experiment_name} --logging-dir={EXPERIMENTS_FOLDER_URI} --num-workers={WORKER_NODE_COUNT} --use-gpu\"\n",
    "train_experiment_uri = EXPERIMENTS_FOLDER_URI / train_experiment_name\n",
    "train_runtime_env = {\n",
    "    \"working_dir\": str(src_path),\n",
    "    \"env_vars\": {\"HF_TOKEN\": HF_TOKEN, \"TORCH_NCCL_ASYNC_ERROR_HANDLING\": \"3\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhwkH-NzgEHS"
   },
   "source": [
    "Submit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUPB6YKpur4f"
   },
   "outputs": [],
   "source": [
    "train_job_id = client.submit_job(\n",
    "    submission_id=train_submission_id,\n",
    "    entrypoint=train_entrypoint,\n",
    "    runtime_env=train_runtime_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T0MaYZegF_1"
   },
   "source": [
    "Check the status of the job while is running using the `monitor_job` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uCeasefuso8"
   },
   "outputs": [],
   "source": [
    "train_job_status = monitor_job(client, train_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67FI31SqKFdF"
   },
   "source": [
    "### Check training artifacts\n",
    "\n",
    "After the Ray training job has completed, see the model artifacts in the Cloud Storage location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_JOYhXQKK8U"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -l {train_experiment_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAOAsmGYZ2l9"
   },
   "source": [
    "### Log metrics in Vertex AI TensorBoard\n",
    "\n",
    "Use Vertex AI TensorBoard for validating your training job by logging resulting metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSMCKEp9Z8jx"
   },
   "outputs": [],
   "source": [
    "vertex_ai.upload_tb_log(\n",
    "    tensorboard_id=tensorboard.name,\n",
    "    tensorboard_experiment_name=train_experiment_name,\n",
    "    logdir=str(train_experiment_uri),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2RfyLpW0XO7"
   },
   "source": [
    "## Serving tuned Gemma model with Ray Data for offline predictions\n",
    "\n",
    "Using Ray on Vertex AI for developing AI/ML applications offers various benefits. In this scenario, you can use Cloud storage to conveniently store model checkpoints, metrics and more. This allows you to quickly consume the model for AI/ML downstreaming tasks including generating batch predictions using Ray Data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "700NkAsLQHJI"
   },
   "source": [
    "### Generate predictions (locally)\n",
    "\n",
    "Generate predictions locally to validate the tuned model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN5YQ60PXbYq"
   },
   "source": [
    "#### Download Ray training checkpoints\n",
    "\n",
    "Download all resulting checkpoints from Ray job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBf0_agLhrLV"
   },
   "outputs": [],
   "source": [
    "! gsutil -q cp -r {train_experiment_uri}/* {experiments_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VavpxaY1hlSN"
   },
   "source": [
    "#### Get the best checkpoint\n",
    "\n",
    "Use the `ExperimentAnalysis` method to retrieve the the best checkpoint according to relevant metrics and mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUBEKpK1gVx9"
   },
   "outputs": [],
   "source": [
    "experiment_analysis = ExperimentAnalysis(experiments_path)\n",
    "log_path = experiment_analysis.get_best_trial(metric=\"eval_rougeLsum\", mode=\"max\")\n",
    "best_checkpoint = experiment_analysis.get_best_checkpoint(\n",
    "    log_path, metric=\"eval_rougeLsum\", mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icFuBCwJXSub"
   },
   "source": [
    "#### Load the model after training\n",
    "\n",
    "After training the model, load the model as described in the Hugging Face [documentation](https://huggingface.co/docs/trl/use_model#use-adapters-peft).\n",
    "\n",
    "Set the model and adapters path. Also set the path to store the resulting tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "370W-GHE7vM2"
   },
   "outputs": [],
   "source": [
    "base_model_path = \"google/gemma-2b-it\"\n",
    "peft_model_path = epath.Path(best_checkpoint.path) / \"checkpoint\"\n",
    "tuned_model_path = models_path / \"xsum-tuned-gemma-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UCc4hvGjES8"
   },
   "source": [
    "Initiate the associated Gemma tokenizer and base model. Also initiate the resulting adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbctXuXQAhLP"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    peft_model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    is_trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Jytit8inMGy"
   },
   "source": [
    "Merge the base model and adapters to save the tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRGvb7hRnK-a"
   },
   "outputs": [],
   "source": [
    "tuned_model = peft_model.merge_and_unload()\n",
    "tuned_model.save_pretrained(tuned_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4WRLix49saQ"
   },
   "source": [
    "#### Generate summaries\n",
    "\n",
    "Generate summaries with the tuned model. Load the validation set of the tutorial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrsR5HjNunzW"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"xsum\", split=\"validation\", cache_dir=data_path, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm5eFJNxoAHk"
   },
   "source": [
    "Sample one article to summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZjT8zAOPivt"
   },
   "outputs": [],
   "source": [
    "sample = dataset.select([random.randint(0, len(dataset) - 1)])\n",
    "document = sample[\"document\"][0]\n",
    "reference_summary = sample[\"summary\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16GiJy_EoEgi"
   },
   "source": [
    "Prepare the associated prompt following the [Gemma documentation](https://ai.google.dev/gemma/docs/formatting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFhA8R3wXPKj"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Summarize the following ARTICLE in one sentence.\\\\n###ARTICLE: {document}\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7Lvss80objd"
   },
   "source": [
    "Initiate the text-generation pipeline for generating summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wq6m6G-b_w7b"
   },
   "outputs": [],
   "source": [
    "tuned_gemma_pipeline = pipeline(\n",
    "    \"text-generation\", model=tuned_model, tokenizer=tokenizer, max_new_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb5GNgv1ovpc"
   },
   "source": [
    "Generate the associated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuF2CMGx6AGi"
   },
   "outputs": [],
   "source": [
    "generated_tuned_gemma_summary = tuned_gemma_pipeline(\n",
    "    prompt, do_sample=True, temperature=0.1, add_special_tokens=True\n",
    ")[0][\"generated_text\"][len(prompt) :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKjU_Mhgo8yO"
   },
   "source": [
    "Print the generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oq6HLy4Pu8oC"
   },
   "outputs": [],
   "source": [
    "print(f\"Reference summary: {reference_summary}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Tuned generated summary: {generated_tuned_gemma_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdhPQC5aKmyP"
   },
   "source": [
    "#### Evaluate models\n",
    "\n",
    "As an additional step, you can evaluate the tuned model. To evaluate the model you compare models qualitatively and quantitatively.\n",
    "\n",
    "In one case, you compare responses generated by the base Gemma model with the ones generated by the tuned Gemma model. In the other case, you calculate ROUGE metrics and its improvements which gives you an idea of how well the tuned models is able to reproduce the reference summaries correctly with respect to the base model.\n",
    "\n",
    "Evaluate models by comparing generated summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmbCCJpWora7"
   },
   "outputs": [],
   "source": [
    "gemma_pipeline = pipeline(\n",
    "    \"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=50\n",
    ")\n",
    "\n",
    "generated_gemma_summary = gemma_pipeline(\n",
    "    prompt, do_sample=True, temperature=0.1, add_special_tokens=True\n",
    ")[0][\"generated_text\"][len(prompt) :]\n",
    "\n",
    "print(f\"Reference summary: {reference_summary}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Base generated summary: {generated_gemma_summary}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Tuned generated summary: {generated_tuned_gemma_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLt-3ovZq7y1"
   },
   "source": [
    "Evaluate models by computing ROUGE metrics and its improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRSPxJVpfJ3E"
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEO5qqtsMC75"
   },
   "outputs": [],
   "source": [
    "gemma_results = rouge.compute(\n",
    "    predictions=[generated_gemma_summary],\n",
    "    references=[reference_summary],\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYC5t4I594pn"
   },
   "outputs": [],
   "source": [
    "tuned_gemma_results = rouge.compute(\n",
    "    predictions=[generated_tuned_gemma_summary],\n",
    "    references=[reference_summary],\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnWVuRe394gh"
   },
   "outputs": [],
   "source": [
    "improvements = {}\n",
    "for rouge_metric, gemma_rouge in gemma_results.items():\n",
    "    tuned_gemma_rouge = tuned_gemma_results[rouge_metric]\n",
    "    if gemma_rouge != 0:\n",
    "        improvement = ((tuned_gemma_rouge - gemma_rouge) / gemma_rouge) * 100\n",
    "    else:\n",
    "        improvement = None\n",
    "    improvements[rouge_metric] = improvement\n",
    "\n",
    "print(\"Base Gemma vs Tuned Gemma - ROUGE improvements\")\n",
    "for rouge_metric, improvement in improvements.items():\n",
    "    print(f\"{rouge_metric}: {improvement:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G51Xhb4H0fa1"
   },
   "source": [
    "### Batch prediction with Ray Data\n",
    "\n",
    "To generate batch prediction with the tuned model using Ray Data on Ray on Vertex AI, you need a dataset to generate predictions and the tuned model stored in the Cloud bucket.\n",
    "\n",
    "Then, you can leverage Ray Data which provides an easy-to-use API for offline batch inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjV1oHCTCe1r"
   },
   "source": [
    "#### Upload the tuned model\n",
    "\n",
    "Upload the tuned model on the Cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFwD2p8jCjgH"
   },
   "outputs": [],
   "source": [
    "! gsutil -q cp -r {models_path} {MODELS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIwOKSnrpxkW"
   },
   "source": [
    "#### Prepare the batch prediction training script\n",
    "\n",
    "Prepare `src/batch_predict.py` file which is the Python script for executing the Ray batch prediction job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekL5tV31pxkX"
   },
   "outputs": [],
   "source": [
    "batch_predictor_script = \"\"\"\n",
    "# General\n",
    "import argparse\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Serving\n",
    "import datasets\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.pipelines import pipeline\n",
    "\n",
    "# Ray\n",
    "import ray\n",
    "\n",
    "# Settings\n",
    "datasets.disable_progress_bar()\n",
    "\n",
    "# Variables\n",
    "base_model_path = \"google/gemma-2b-it\"\n",
    "\n",
    "\n",
    "# helpers\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Batch prediction with Gemma on Ray on Vertex AI')\n",
    "    parser.add_argument('--tuned_model_path', type=str, help='path of adapter model')\n",
    "    parser.add_argument('--num_gpus', type=int, default=1, help='number of gpus')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='batch size')\n",
    "    parser.add_argument('--sample_size', type=int, default=20, help='number of articles to summarize')\n",
    "    parser.add_argument('--temperature', type=float, default=0.1, help='temperature for generating summaries')\n",
    "    parser.add_argument('--max_new_tokens', type=int, default=50, help='max new token for generating summaries')\n",
    "    parser.add_argument('--output_dir', type=str, help='output directory for predictions')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Set configuration\n",
    "    args = get_args()\n",
    "    config = vars(args)\n",
    "\n",
    "    # Setting training\n",
    "    login(token=os.environ['HF_TOKEN'], add_to_git_credential=True)\n",
    "    transformers.set_seed(8)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset_id = \"xsum\"\n",
    "    sample_size = config[\"sample_size\"]\n",
    "    input_data = datasets.load_dataset(dataset_id, split=\"validation\", trust_remote_code=True)\n",
    "    input_data = input_data.select(range(sample_size))\n",
    "    ray_input_data = ray.data.from_huggingface(input_data)\n",
    "\n",
    "    # Generate predictions\n",
    "\n",
    "    class Summarizer:\n",
    "\n",
    "      def __init__(self):\n",
    "          self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "          self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "          self.tuned_model = AutoModelForCausalLM.from_pretrained(config[\"tuned_model_path\"],\n",
    "                                                                  device_map='auto',\n",
    "                                                                  torch_dtype=torch.float16)\n",
    "\n",
    "          self.pipeline = pipeline(\"text-generation\",\n",
    "                                    model=self.tuned_model,\n",
    "                                    tokenizer=self.tokenizer,\n",
    "                                    max_new_tokens=config[\"max_new_tokens\"])\n",
    "\n",
    "      def __call__(self, batch: np.ndarray):\n",
    "\n",
    "          # prepare dataset\n",
    "          messages = [{\"role\": \"user\",\n",
    "                      \"content\": f\"Summarize the following ARTICLE in one sentence.\\\\n###ARTICLE: {document}\"}\n",
    "                      for document in batch[\"document\"]]\n",
    "\n",
    "          batch['prompt'] = [self.tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
    "                             for message in messages]\n",
    "\n",
    "          # generate\n",
    "          batch['generated_summary'] = [self.pipeline(prompt,\n",
    "                                                    do_sample=True,\n",
    "                                                    temperature=config[\"temperature\"],\n",
    "                                                    add_special_tokens=True)[0][\"generated_text\"][len(prompt):]\n",
    "                                                    for prompt in batch['prompt']]\n",
    "\n",
    "          return batch\n",
    "\n",
    "\n",
    "    predictions_data = ray_input_data.map_batches(\n",
    "        Summarizer,\n",
    "        concurrency=config[\"num_gpus\"],\n",
    "        num_gpus=1,\n",
    "        batch_size=config['batch_size'])\n",
    "\n",
    "    # Store resulting predictions\n",
    "    predictions_data.write_json(config[\"output_dir\"], try_create_dir=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(src_path / \"batch_predictor.py\", \"w\") as f:\n",
    "    f.write(batch_predictor_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xynt8impxkX"
   },
   "source": [
    "####  Submit a Ray job using the Ray Jobs API\n",
    "\n",
    "Submit the script to the Ray on Vertex AI cluster using the Ray Jobs API via  the public Ray dashboard address.\n",
    "\n",
    "Initiate the client to submit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hY96qSQFpxkX"
   },
   "outputs": [],
   "source": [
    "client = JobSubmissionClient(\n",
    "    address=\"vertex_ray://{}\".format(ray_cluster.dashboard_address)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ6EqvIF5q9F"
   },
   "source": [
    "Set some job configuration including model path, job id, prediction entrypoint and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDycAWy45siE"
   },
   "outputs": [],
   "source": [
    "batch_predict_id = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "batch_predict_submission_id = f\"ray-job-{batch_predict_id}\"\n",
    "tuned_model_uri_path = str(MODELS_PATH / \"xsum-tuned-gemma-it\").replace(\n",
    "    \"gs://\", \"/gcs/\"\n",
    ")\n",
    "batch_predict_entrypoint = f\"python3 batch_predictor.py --tuned_model_path={tuned_model_uri_path} --num_gpus=2 --output_dir={PREDICTIONS_FOLDER_URI}\"\n",
    "batch_predict_runtime_env = {\n",
    "    \"working_dir\": str(src_path),\n",
    "    \"env_vars\": {\"HF_TOKEN\": HF_TOKEN},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcaMEKz6pxkX"
   },
   "source": [
    "Submit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qADWE5LOpxkX"
   },
   "outputs": [],
   "source": [
    "batch_predict_job_id = client.submit_job(\n",
    "    submission_id=batch_predict_submission_id,\n",
    "    entrypoint=batch_predict_entrypoint,\n",
    "    runtime_env=batch_predict_runtime_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxrOwuVf6f3R"
   },
   "source": [
    "Check the status of the job using the `monitor_job` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXTF8pp9KJS8"
   },
   "outputs": [],
   "source": [
    "batch_predict_job_status = monitor_job(client, batch_predict_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2Iafitj7wtH"
   },
   "source": [
    "#### Get generated summaries\n",
    "\n",
    "Have a quick view of generated summaries using a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em4WQokY7050"
   },
   "outputs": [],
   "source": [
    "predictions_df = read_json_files(prefix=\"predictions/\", bucket_name=BUCKET_NAME)\n",
    "predictions_df = predictions_df[\n",
    "    [\"id\", \"document\", \"prompt\", \"summary\", \"generated_summary\"]\n",
    "]\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) that you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources that you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delete_tensorboards = False\n",
    "delete_experiments = False\n",
    "delete_ray_clusters = False\n",
    "delete_image_repo = False\n",
    "delete_bucket = False\n",
    "delete_tutorial = False\n",
    "\n",
    "# Delete tensorboard\n",
    "if delete_tensorboards:\n",
    "    tensorboard_list = vertex_ai.Tensorboard.list()\n",
    "    for tensorboard in tensorboard_list:\n",
    "        tensorboard.delete()\n",
    "\n",
    "# Delete experiments\n",
    "if delete_experiments:\n",
    "    experiment_list = vertex_ai.Experiment.list()\n",
    "    for experiment in experiment_list:\n",
    "        experiment.delete()\n",
    "\n",
    "# Delete ray on vertex cluster\n",
    "if delete_ray_clusters:\n",
    "    ray_cluster_list = vertex_ray.list_ray_clusters()\n",
    "    for ray_cluster in ray_cluster_list:\n",
    "        vertex_ray.delete_ray_cluster(ray_cluster.cluster_resource_name)\n",
    "\n",
    "if delete_image_repo:\n",
    "    ! gcloud artifacts repositories delete {REPO_NAME}\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "if delete_bucket:\n",
    "    ! gsutil -q -m rm -r {BUCKET_URI}\n",
    "\n",
    "# Delete tutorial folder\n",
    "if delete_tutorial:\n",
    "    shutil.rmtree(tutorial_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

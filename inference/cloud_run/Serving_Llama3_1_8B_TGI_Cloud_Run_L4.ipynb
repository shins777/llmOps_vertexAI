{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Forusone\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Serving-hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4-TGI-Cloud Run-L4\n",
    "* [Hugging Face TGI Metrics](https://huggingface.co/docs/text-generation-inference/en/reference/metrics)\n",
    "* [Run LLM inference on Cloud Run GPUs with Hugging Face TGI (services)](https://cloud.google.com/run/docs/tutorials/gpu-llama3-with-tgi)\n",
    "* [Deploy Meta Llama 3.1 8B with TGI DLC on Cloud Run](https://huggingface.co/docs/google-cloud/examples/cloud-run-tgi-deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eaqts7U9Q1-h"
   },
   "outputs": [],
   "source": [
    "# @title Define deployment constants\n",
    "PROJECT_ID=\"ai-hangsik\" # @param {type:\"string\"}\n",
    "LOCATION=\"us-central1\"  # @param {type:\"string\"}\n",
    "CONTAINER_URI=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-4.ubuntu2204.py311\" # @param {type:\"string\"}\n",
    "SERVICE_NAME=\"hf-tgi-llama31-8b\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "os9UHzadQ17p",
    "outputId": "141403db-79a1-466a-f46c-d7b0f29e8c64"
   },
   "outputs": [],
   "source": [
    "# # @title Authentication\n",
    "# !gcloud auth login\n",
    "# !gcloud auth application-default login\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjJA64xDQ145"
   },
   "outputs": [],
   "source": [
    "# @title Enable Cloud Run APIs\n",
    "!gcloud services enable run.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxxABdgfiUzU"
   },
   "source": [
    "## Deploy a cloud run with a model\n",
    "* Need to request Nvidia L4 GPU for Cloud Run. [Quota increase](https://cloud.google.com/run/quotas#increase).\n",
    "* [TGI launcher arguments](https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/launcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JaP1z7aQ11x",
    "outputId": "72757e13-60ee-4037-ddea-6e37f428c55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying container to Cloud Run service [\u001b[1mhf-tgi-llama31-8b\u001b[m] in project [\u001b[1mai-hangsik\u001b[m] region [\u001b[1mus-central1\u001b[m]\n",
      "Service [\u001b[1mhf-tgi-llama31-8b\u001b[m] revision [\u001b[1mhf-tgi-llama31-8b-00001-q2c\u001b[m] has been deployed and is serving \u001b[1m100\u001b[m percent of traffic.\n",
      "Service URL: \u001b[1mhttps://hf-tgi-llama31-8b-721521243942.us-central1.run.app\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# @title Cloud run command to deploy a model.\n",
    "!gcloud beta run deploy $SERVICE_NAME \\\n",
    "    --image=$CONTAINER_URI \\\n",
    "    --args=\"--model-id=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4,--quantize=awq,--max-concurrent-requests=64\" \\\n",
    "    --port=8080 \\\n",
    "    --cpu=4 \\\n",
    "    --memory=16Gi \\\n",
    "    --no-cpu-throttling \\\n",
    "    --gpu=1 \\\n",
    "    --gpu-type=nvidia-l4 \\\n",
    "    --max-instances=1 \\\n",
    "    --concurrency=64 \\\n",
    "    --region={LOCATION} \\\n",
    "    --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1ZEcSEGjGur"
   },
   "source": [
    "## Run a demo using Cloud Run proxy on local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hL91IGkjLr-"
   },
   "source": [
    "* Athenticate\n",
    "```\n",
    "gcloud auth login\n",
    "```\n",
    "\n",
    "* Execute the follwing command on your local machine.\n",
    "```\n",
    "gcloud run services proxy $SERVICE_NAME --region $LOCATION\n",
    " --> gcloud run services proxy hf-tgi-llama31-8b --port 8088 --region us-central1\n",
    "```\n",
    "* You can see the following information.\n",
    "```\n",
    "/Users/hangsik$ gcloud run services proxy hf-tgi-llama31-8b --port 8088 --region us-central1\n",
    "Proxying to Cloud Run service [hf-tgi-llama31-8b] in project [ai-hangsik] region [us-central1]\n",
    "http://127.0.0.1:8088 proxies to https://hf-tgi-llama31-8b-o5gpdmpuwq-uc.a.run.app\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qHdDmUpk9BL"
   },
   "source": [
    "* Execute the following command on your local machine.\n",
    "```\n",
    "curl http://localhost:8088/v1/chat/completions \\\n",
    "    -X POST \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{\n",
    "        \"model\": \"tgi\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is Deep Learning?\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 128\n",
    "    }'\n",
    "```\n",
    "\n",
    "* Response\n",
    "```\n",
    "{\"object\":\"chat.completion\",\"id\":\"\",\"created\":1736821056,\"model\":\"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\",\"system_fingerprint\":\"2.4.0-native\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Deep Learning is a subfield of machine learning (a subset of artificial intelligence) that uses multi-layered artificial neural networks to analyze and learn from data. These neural networks are inspired by the structure and function of the human brain, where connections between layers of neurons enable complex patterns to be recognized and learned.\\n\\nOver the last decade, Deep Learning has gained tremendous popularity due to its remarkable performance in various applications such as:\\n\\n1. **Image Classification**: Self-driving cars, facial recognition, object detection in video footage.\\n2. **Natural Language Processing (NLP)**: Sentiment analysis, language translation, text summarization, chatbots.\\n3\"},\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":{\"prompt_tokens\":46,\"completion_tokens\":128,\"total_tokens\":174}}/Users/hangsik$\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
